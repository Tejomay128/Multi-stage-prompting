{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc981e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from multi_stage import LLM\n",
    "from prep_data import get_eng_hi_dataset\n",
    "from transformers import GPT2LMHeadModel, MT5Tokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d82686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prefix(nn.Module):\n",
    "    def __init__(self, config, n_prefixes, len_prefix) -> None:\n",
    "        super(Prefix, self).__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.embed_size = config.hidden_size\n",
    "        self.n_layers = config.num_hidden_layers\n",
    "        self.n_heads = config.num_attention_heads\n",
    "        self.head_size = self.embed_size // self.n_heads \n",
    "\n",
    "        self.reparams = nn.Parameter(torch.ones(len_prefix))   \n",
    "        self.prefixes = nn.Parameter(torch.empty((n_prefixes, 2 * self.n_layers, len_prefix, self.hidden_size)))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            nn.init.xavier_uniform_(self.prefixes)\n",
    "        \n",
    "    def forward(self, batch_size):\n",
    "        interm = torch.maximum(torch.ones(1, device=self.reparams.device), self.reparams)\n",
    "        interm = interm.view([1, 1, interm.shape[0], 1])\n",
    "        # print(interm.shape, self.prefixes[:, 0::2, :, :].shape)\n",
    "        keys = (self.prefixes[:, 0::2, :, :] * interm).repeat([batch_size, 1, 1, 1, 1])\n",
    "        values = (self.prefixes[:, 1::2, :, :] * interm).repeat([batch_size, 1, 1, 1, 1])\n",
    "        keys = keys.view(keys.shape[0], keys.shape[1], keys.shape[2], keys.shape[3], self.n_heads, self.head_size)\n",
    "        values = values.view(values.shape[0], values.shape[1], values.shape[2], values.shape[3], self.n_heads, self.head_size)\n",
    "\n",
    "        # keys/values: (batch_size, n_prefixes ,n_layers, len_prefix, n_heads, head_size)\n",
    "\n",
    "        return keys, values\n",
    "    \n",
    "\n",
    "class LLM(nn.Module):\n",
    "    def __init__(self, model, len_prefix) -> None:\n",
    "        super(LLM, self).__init__()\n",
    "        self._model = model\n",
    "        self.hidden_size = model.config.hidden_size\n",
    "        self.embed_size = model.config.hidden_size\n",
    "        self.n_layers = model.config.num_hidden_layers\n",
    "        self.n_heads = model.config.num_attention_heads\n",
    "        self.head_size = self.embed_size // self.n_heads\n",
    "        self.len_prefix = len_prefix\n",
    "\n",
    "        self.prefix = Prefix(model.config, 1, len_prefix)\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=2)\n",
    "        self.logSoftmax_1 = nn.LogSoftmax(dim=1)\n",
    "        self.nll = nn.NLLLoss()\n",
    "        \n",
    "        for param in self._model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "\n",
    "    def encode(self, input_ids, input_mask):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        len_prefix = self.len_prefix\n",
    "        len_sent = input_ids.shape[1]\n",
    "        prefix_mask = torch.ones([batch_size, len_prefix], device=input_mask.device)\n",
    "        attn_mask = torch.cat([prefix_mask, input_mask], dim=1)\n",
    "#         pos_ids = torch.arange(0, input_ids.shape[-1], dtype=torch.long, device=input_mask.device)\n",
    "#         pos_ids = pos_ids.unsqueeze(0).view(-1, input_ids.shape[-1])\n",
    "\n",
    "        keys, values = self.prefix(batch_size)\n",
    "        # batch_size, n_prefixes, n_layers, len_prefix, n_heads, head_size => batch_size, n_heads, len_prefix, head_size\n",
    "\n",
    "        layer_prefix_list = []\n",
    "        for i in range(self.n_layers):\n",
    "            tup = (keys[:,0,i,:,:,:].permute(0,2,1,3), values[:,0,i,:,:,:].permute(0,2,1,3))\n",
    "            layer_prefix_list.append(tup)\n",
    "\n",
    "        outputs = self._model(input_ids, \n",
    "                              past_key_values=layer_prefix_list, \n",
    "                              attention_mask=attn_mask,  \n",
    "                              use_cache=True)\n",
    "\n",
    "        #RE-ENCODING\n",
    "        layer_prefix_list = []\n",
    "        attn_mask = torch.cat([input_mask, input_mask], dim=1)\n",
    "#         pos_ids = torch.arange(0, input_ids.shape[-1], dtype=torch.long, device=input_mask.device)\n",
    "#         pos_ids = pos_ids.unsqueeze(0).view(-1, input_ids.shape[-1])\n",
    "\n",
    "        #prepend re-encoding prefixes and exclude encoder prefixes in the re-encoding stage\n",
    "        for i, (key, value) in enumerate(outputs.past_key_values):\n",
    "            k = key[:,:,len_prefix:,:]\n",
    "            v = value[:,:,len_prefix:,:]\n",
    "            layer_prefix_list.append((k, v))\n",
    "        \n",
    "        outputs = self._model(input_ids, \n",
    "                              past_key_values=layer_prefix_list, \n",
    "                              attention_mask=attn_mask, \n",
    "                              use_cache=True)\n",
    "        layer_prefix_list = []\n",
    "\n",
    "        #prepare past key values for decoding stage\n",
    "        for i, (key, value) in enumerate(outputs.past_key_values):\n",
    "            k = key[:,:,len_sent:,:]\n",
    "            v = value[:,:,len_sent:,:]\n",
    "            layer_prefix_list.append((k, v))\n",
    "\n",
    "        return layer_prefix_list\n",
    "    \n",
    "    def decode(self, target_ids, input_mask, target_mask, past_key_values, mode='train'):\n",
    "        batch_size = target_ids.shape[0]\n",
    "        len_prefix = self.len_prefix\n",
    "        prefix_mask = torch.ones([batch_size, len_prefix], device=input_mask.device)\n",
    "\n",
    "        #only attend to decode stage prefixes and re-encoding stage hidden states\n",
    "        attn_mask = torch.cat([input_mask, target_mask], dim=1)\n",
    "#         pos_ids = torch.arange(0, target_ids.shape[-1], dtype=torch.long, device=input_mask.device)\n",
    "#         pos_ids = pos_ids.unsqueeze(0).view(-1, target_ids.shape[-1])\n",
    "\n",
    "        outputs = self._model(target_ids, \n",
    "                              past_key_values=past_key_values, \n",
    "                              attention_mask=attn_mask,  \n",
    "                              use_cache=True)\n",
    "        return outputs.logits, outputs.past_key_values\n",
    "    \n",
    "    def forward(self, input_ids, input_mask, target_ids, target_mask):\n",
    "        past_key_values = self.encode(input_ids, input_mask)\n",
    "        labels = target_ids[:, 1:]\n",
    "        target_ids = target_ids[:, :-1]\n",
    "        target_mask = target_mask[:, :-1]\n",
    "        logits,_ = self.decode(target_ids, input_mask, target_mask, past_key_values)\n",
    "\n",
    "        # make batch size and sentence length as one dimension\n",
    "        logprobs = self.logSoftmax(logits)\n",
    "        logprobs = logprobs.reshape([logprobs.shape[0] * logprobs.shape[1], -1])\n",
    "        target_mask = target_mask.reshape([target_mask.shape[0] * target_mask.shape[1],])\n",
    "        labels = labels.flatten()\n",
    "        loss = -logprobs[torch.arange(logprobs.shape[0], device=labels.device), labels]\n",
    "#         print(loss.shape, target_mask.shape)\n",
    "        loss = torch.sum(loss * target_mask) / torch.sum(target_mask)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d286b458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/cfilt___parquet/cfilt--iitb-english-hindi-911387c6837f8b91/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf89aaebaf84307b791293dd3ab599e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_data(PATH):\n",
    "    dataset = []\n",
    "    f_en = open(PATH + 'filtered.en', 'r')\n",
    "    for line in f_en.readlines():\n",
    "        line = line.strip('\\n')\n",
    "        entry = {'en': line}\n",
    "        dataset.append(entry)\n",
    "    f_en.close()\n",
    "    \n",
    "    f_hi = open(PATH + 'filtered.hi', 'r')\n",
    "    for i, line in enumerate(f_hi.readlines()):\n",
    "        line = line.strip('\\n')\n",
    "        dataset[i]['hi'] = line\n",
    "    f_hi.close()\n",
    "    return dataset\n",
    "\n",
    "val_data, test_data = get_eng_hi_dataset()\n",
    "train_data = read_data('filtered_data/')\n",
    "train_data = train_data[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6026cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelCorpus(Dataset):\n",
    "    def __init__(self, data, src_lang='en', tgt_lang='hi') -> None:\n",
    "        super(ParallelCorpus, self).__init__()\n",
    "        self.src = []\n",
    "        self.tgt = []\n",
    "        for pair in data:\n",
    "            self.src.append(pair[src_lang])\n",
    "            self.tgt.append(pair[tgt_lang])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.tgt[index]\n",
    "\n",
    "train_pc = ParallelCorpus(train_data, src_lang='en', tgt_lang='hi')\n",
    "test_pc = ParallelCorpus(test_data, src_lang='en', tgt_lang='hi')\n",
    "val_pc = ParallelCorpus(val_data, src_lang='en', tgt_lang='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e025f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_prefix = 100\n",
    "lr = 1e-4\n",
    "batch_size = 4\n",
    "num_epochs = 2\n",
    "token_limit = ((1023 - len_prefix) // 2) - 3  #to accomodate extra one token if max_len=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c447662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_pc, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_pc, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_pc, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8b045b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'T5Tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MT5Tokenizer.from_pretrained(\"THUMT/mGPT\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"THUMT/mGPT\")\n",
    "\n",
    "MT_model = LLM(model, len_prefix).to(device)\n",
    "optimizer = torch.optim.Adam(params=MT_model.parameters(),lr=lr, eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a9879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------EPOCH 1-------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation():\n",
    "    total_loss = 0\n",
    "    for i, (src, tgt) in enumerate(val_loader):\n",
    "        max_src_len = min(token_limit, max([len(s) for s in src])) + 1   #need this to accomodate max_len = 1\n",
    "        max_tgt_len = min(token_limit, max([len(s) for s in tgt])) + 1\n",
    "        inputs = tokenizer(src, padding='max_length', truncation=True, max_length=max_src_len)\n",
    "        targets = tokenizer(tgt, padding='max_length', truncation=True, max_length=max_tgt_len)\n",
    "        input_ids, input_masks = inputs['input_ids'], inputs['attention_mask']\n",
    "        target_ids, target_masks = targets['input_ids'], targets['attention_mask']\n",
    "        for j in range(len(target_ids)):\n",
    "            target_ids[j].insert(0, 1)\n",
    "            target_masks[j].insert(0, 1)\n",
    "        input_ids, input_masks = torch.tensor(input_ids).to(device), torch.tensor(input_masks).to(device)\n",
    "        target_ids, target_masks = torch.tensor(target_ids).to(device), torch.tensor(target_masks).to(device)\n",
    "        loss = MT_model(input_ids, input_masks, target_ids, target_masks)\n",
    "        total_loss += loss\n",
    "    return total_loss / len(val_loader)\n",
    "        \n",
    "\n",
    "min_val_loss = 10000\n",
    "PATH = 'saved_models/finetune.pt'\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"------------------------EPOCH {epoch + 1}-------------------------------\")\n",
    "    t1 = time.time()\n",
    "    for i, (src, tgt) in enumerate(train_loader):\n",
    "        MT_model.zero_grad()\n",
    "        \n",
    "        max_src_len = min(token_limit, max([len(s) for s in src])) + 1   #need this to accomodate max_len = 1\n",
    "        max_tgt_len = min(token_limit, max([len(s) for s in tgt])) + 1\n",
    "        inputs = tokenizer(src, padding='max_length', truncation=True, max_length=max_src_len)\n",
    "        targets = tokenizer(tgt, padding='max_length', truncation=True, max_length=max_tgt_len)\n",
    "        input_ids, input_masks = inputs['input_ids'], inputs['attention_mask']\n",
    "        target_ids, target_masks = targets['input_ids'], targets['attention_mask']\n",
    "#         print(len(input_ids[0]))\n",
    "        for j in range(len(target_ids)):\n",
    "            target_ids[j].insert(0, 1)\n",
    "            target_masks[j].insert(0, 1)\n",
    "#         print(len(input_ids[0]))\n",
    "#         print(MT_model._model.config.max_position_embeddings)\n",
    "#         print(tgt[0], target_ids[0])\n",
    "        \n",
    "        input_ids, input_masks = torch.tensor(input_ids).to(device), torch.tensor(input_masks).to(device)\n",
    "        target_ids, target_masks = torch.tensor(target_ids).to(device), torch.tensor(target_masks).to(device)\n",
    "        loss = MT_model(input_ids, input_masks, target_ids, target_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1)%500 == 0:\n",
    "            t2 = time.time()\n",
    "            val_loss = validation()\n",
    "            if val_loss.item() < min_val_loss:\n",
    "                torch.save(MT_model.state_dict(), PATH)\n",
    "                min_val_loss = val_loss\n",
    "            print(f'Step {i+1} | Val Loss: {val_loss.item():.5f}| Best val loss: {min_val_loss:.5f} | Time: {(t2-t1)/3600 : .4f} hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80212ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5274d881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=6)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from prep_data import get_eng_hi_dataset\n",
    "from transformers import GPT2LMHeadModel, MT5Tokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7e50140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(nn.Module):\n",
    "    def __init__(self, model) -> None:\n",
    "        super(LLM, self).__init__()\n",
    "        self._model = model\n",
    "        self.hidden_size = model.config.hidden_size\n",
    "        self.embed_size = model.config.hidden_size\n",
    "        self.n_layers = model.config.num_hidden_layers\n",
    "        self.n_heads = model.config.num_attention_heads\n",
    "        self.head_size = self.embed_size // self.n_heads\n",
    "\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=2)\n",
    "        self.logSoftmax_1 = nn.LogSoftmax(dim=1)\n",
    "        self.nll = nn.NLLLoss()\n",
    "\n",
    "\n",
    "    def encode(self, input_ids, input_mask):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        len_sent = input_ids.shape[1]\n",
    "        attn_mask = input_mask\n",
    "\n",
    "        outputs = self._model(input_ids, \n",
    "                              attention_mask=attn_mask,  \n",
    "                              use_cache=True)\n",
    "\n",
    "        #RE-ENCODING\n",
    "        layer_prefix_list = []\n",
    "        attn_mask = torch.cat([input_mask, input_mask], dim=1)\n",
    "\n",
    "        outputs = self._model(input_ids, \n",
    "                              past_key_values=outputs.past_key_values, \n",
    "                              attention_mask=attn_mask, \n",
    "                              use_cache=True)\n",
    "        \n",
    "        past_key_values = []\n",
    "        for (key, value) in outputs.past_key_values:\n",
    "            k = key[:,:,len_sent:,:]\n",
    "            v = value[:,:,len_sent:,:]\n",
    "            past_key_values.append((k, v))\n",
    "\n",
    "        return past_key_values\n",
    "    \n",
    "    def decode(self, target_ids, input_mask, target_mask, past_key_values, mode='train'):\n",
    "        batch_size = target_ids.shape[0]\n",
    "        attn_mask = torch.cat([input_mask, target_mask], dim=1)\n",
    "\n",
    "        outputs = self._model(target_ids, \n",
    "                              past_key_values=past_key_values, \n",
    "                              attention_mask=attn_mask,  \n",
    "                              use_cache=True)\n",
    "        return outputs.logits, outputs.past_key_values\n",
    "    \n",
    "    def forward(self, input_ids, input_mask, target_ids, target_mask):\n",
    "        past_key_values = self.encode(input_ids, input_mask)\n",
    "        labels = target_ids[:, 1:]\n",
    "        target_ids = target_ids[:, :-1]\n",
    "        target_mask = target_mask[:, :-1]\n",
    "        logits,_ = self.decode(target_ids, input_mask, target_mask, past_key_values)\n",
    "\n",
    "        # make batch size and sentence length as one dimension\n",
    "        logprobs = self.logSoftmax(logits)\n",
    "        logprobs = logprobs.reshape([logprobs.shape[0] * logprobs.shape[1], -1])\n",
    "        target_mask = target_mask.reshape([target_mask.shape[0] * target_mask.shape[1],])\n",
    "        labels = labels.flatten()\n",
    "        loss = -logprobs[torch.arange(logprobs.shape[0], device=labels.device), labels]\n",
    "#         print(loss.shape, target_mask.shape)\n",
    "        loss = torch.sum(loss * target_mask) / torch.sum(target_mask)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfcd5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/cfilt___parquet/cfilt--iitb-english-hindi-911387c6837f8b91/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a085f2ef09940708980f19990390377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_data(PATH):\n",
    "    dataset = []\n",
    "    f_en = open(PATH + 'filtered.en', 'r')\n",
    "    for line in f_en.readlines():\n",
    "        line = line.strip('\\n')\n",
    "        entry = {'en': line}\n",
    "        dataset.append(entry)\n",
    "    f_en.close()\n",
    "    \n",
    "    f_hi = open(PATH + 'filtered.hi', 'r')\n",
    "    for i, line in enumerate(f_hi.readlines()):\n",
    "        line = line.strip('\\n')\n",
    "        dataset[i]['hi'] = line\n",
    "    f_hi.close()\n",
    "    return dataset\n",
    "\n",
    "val_data, test_data = get_eng_hi_dataset()\n",
    "train_data = read_data('filtered_data/')\n",
    "train_data = train_data[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c40dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelCorpus(Dataset):\n",
    "    def __init__(self, data, src_lang='en', tgt_lang='hi') -> None:\n",
    "        super(ParallelCorpus, self).__init__()\n",
    "        self.src = []\n",
    "        self.tgt = []\n",
    "        for pair in data:\n",
    "            self.src.append(pair[src_lang])\n",
    "            self.tgt.append(pair[tgt_lang])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.tgt[index]\n",
    "\n",
    "train_pc = ParallelCorpus(train_data, src_lang='en', tgt_lang='hi')\n",
    "test_pc = ParallelCorpus(test_data, src_lang='en', tgt_lang='hi')\n",
    "val_pc = ParallelCorpus(val_data, src_lang='en', tgt_lang='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b23c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_prefix = 100\n",
    "lr = 1e-4\n",
    "batch_size = 4\n",
    "num_epochs = 2\n",
    "token_limit = ((1023 - len_prefix) // 2) - 3  #to accomodate extra one token if max_len=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e4c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_pc, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_pc, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_pc, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ffbabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'T5Tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MT5Tokenizer.from_pretrained(\"THUMT/mGPT\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"THUMT/mGPT\")\n",
    "\n",
    "MT_model = LLM(model).to(device)\n",
    "optimizer = torch.optim.Adam(params=MT_model.parameters(),lr=lr, eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b12d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------EPOCH 1-------------------------------\n",
      "Step 500 | Val Loss: 3.35807| Best val loss: 3.35807 | Time:  0.0575 hrs\n",
      "Step 1000 | Val Loss: 3.44138| Best val loss: 3.35807 | Time:  0.1227 hrs\n",
      "Step 1500 | Val Loss: 3.28490| Best val loss: 3.28490 | Time:  0.1880 hrs\n",
      "Step 2000 | Val Loss: 3.36685| Best val loss: 3.28490 | Time:  0.2560 hrs\n",
      "Step 2500 | Val Loss: 3.20250| Best val loss: 3.20250 | Time:  0.3220 hrs\n",
      "Step 3000 | Val Loss: 3.32183| Best val loss: 3.20250 | Time:  0.3891 hrs\n",
      "Step 3500 | Val Loss: 3.18532| Best val loss: 3.18532 | Time:  0.4561 hrs\n",
      "Step 4000 | Val Loss: 3.06976| Best val loss: 3.06976 | Time:  0.5239 hrs\n",
      "Step 4500 | Val Loss: 2.97176| Best val loss: 2.97176 | Time:  0.5931 hrs\n",
      "Step 5000 | Val Loss: 3.05036| Best val loss: 2.97176 | Time:  0.6623 hrs\n",
      "Step 5500 | Val Loss: 2.95531| Best val loss: 2.95531 | Time:  0.7316 hrs\n",
      "Step 6000 | Val Loss: 2.93756| Best val loss: 2.93756 | Time:  0.8049 hrs\n",
      "Step 6500 | Val Loss: 2.93002| Best val loss: 2.93002 | Time:  0.8751 hrs\n",
      "Step 7000 | Val Loss: 2.95588| Best val loss: 2.93002 | Time:  0.9468 hrs\n",
      "Step 7500 | Val Loss: 2.94694| Best val loss: 2.93002 | Time:  1.0160 hrs\n",
      "Step 8000 | Val Loss: 2.94861| Best val loss: 2.93002 | Time:  1.0855 hrs\n",
      "Step 8500 | Val Loss: 2.94061| Best val loss: 2.93002 | Time:  1.1571 hrs\n",
      "Step 9000 | Val Loss: 2.79523| Best val loss: 2.79523 | Time:  1.2339 hrs\n",
      "Step 9500 | Val Loss: 2.87109| Best val loss: 2.79523 | Time:  1.3040 hrs\n",
      "Step 10000 | Val Loss: 2.92027| Best val loss: 2.79523 | Time:  1.3750 hrs\n",
      "Step 10500 | Val Loss: 2.81411| Best val loss: 2.79523 | Time:  1.4457 hrs\n",
      "Step 11000 | Val Loss: 2.77896| Best val loss: 2.77896 | Time:  1.5169 hrs\n",
      "Step 11500 | Val Loss: 2.78643| Best val loss: 2.77896 | Time:  1.5901 hrs\n",
      "Step 12000 | Val Loss: 2.75958| Best val loss: 2.75958 | Time:  1.6618 hrs\n",
      "Step 12500 | Val Loss: 2.74054| Best val loss: 2.74054 | Time:  1.7358 hrs\n",
      "Step 13000 | Val Loss: 2.73694| Best val loss: 2.73694 | Time:  1.8105 hrs\n",
      "Step 13500 | Val Loss: 2.78642| Best val loss: 2.73694 | Time:  1.8871 hrs\n",
      "Step 14000 | Val Loss: 2.74195| Best val loss: 2.73694 | Time:  1.9603 hrs\n",
      "Step 14500 | Val Loss: 2.72402| Best val loss: 2.72402 | Time:  2.0355 hrs\n",
      "Step 15000 | Val Loss: 2.73151| Best val loss: 2.72402 | Time:  2.1099 hrs\n",
      "Step 15500 | Val Loss: 2.70830| Best val loss: 2.70830 | Time:  2.1848 hrs\n",
      "Step 16000 | Val Loss: 2.61097| Best val loss: 2.61097 | Time:  2.2617 hrs\n",
      "Step 16500 | Val Loss: 2.64306| Best val loss: 2.61097 | Time:  2.3341 hrs\n",
      "Step 17000 | Val Loss: 2.67702| Best val loss: 2.61097 | Time:  2.4047 hrs\n",
      "Step 17500 | Val Loss: 2.65603| Best val loss: 2.61097 | Time:  2.4779 hrs\n",
      "Step 18000 | Val Loss: 2.67725| Best val loss: 2.61097 | Time:  2.5551 hrs\n",
      "Step 18500 | Val Loss: 2.60041| Best val loss: 2.60041 | Time:  2.6288 hrs\n",
      "Step 19000 | Val Loss: 2.59733| Best val loss: 2.59733 | Time:  2.7048 hrs\n",
      "Step 19500 | Val Loss: 2.61281| Best val loss: 2.59733 | Time:  2.7793 hrs\n",
      "Step 20000 | Val Loss: 2.53379| Best val loss: 2.53379 | Time:  2.8541 hrs\n",
      "Step 20500 | Val Loss: 2.58165| Best val loss: 2.53379 | Time:  2.9293 hrs\n",
      "Step 21000 | Val Loss: 2.54590| Best val loss: 2.53379 | Time:  3.0044 hrs\n",
      "Step 21500 | Val Loss: 2.59035| Best val loss: 2.53379 | Time:  3.0782 hrs\n",
      "Step 22000 | Val Loss: 2.50845| Best val loss: 2.50845 | Time:  3.1532 hrs\n",
      "Step 22500 | Val Loss: 2.58219| Best val loss: 2.50845 | Time:  3.2303 hrs\n",
      "Step 23000 | Val Loss: 2.44223| Best val loss: 2.44223 | Time:  3.3067 hrs\n",
      "Step 23500 | Val Loss: 2.48382| Best val loss: 2.44223 | Time:  3.3855 hrs\n",
      "Step 24000 | Val Loss: 2.44835| Best val loss: 2.44223 | Time:  3.4612 hrs\n",
      "Step 24500 | Val Loss: 2.45489| Best val loss: 2.44223 | Time:  3.5372 hrs\n",
      "Step 25000 | Val Loss: 2.50830| Best val loss: 2.44223 | Time:  3.6120 hrs\n",
      "------------------------EPOCH 2-------------------------------\n",
      "Step 500 | Val Loss: 2.62803| Best val loss: 2.44223 | Time:  0.0573 hrs\n",
      "Step 1000 | Val Loss: 2.62492| Best val loss: 2.44223 | Time:  0.1210 hrs\n",
      "Step 1500 | Val Loss: 2.63288| Best val loss: 2.44223 | Time:  0.1860 hrs\n",
      "Step 2000 | Val Loss: 2.73399| Best val loss: 2.44223 | Time:  0.2530 hrs\n",
      "Step 2500 | Val Loss: 2.65545| Best val loss: 2.44223 | Time:  0.3195 hrs\n",
      "Step 3000 | Val Loss: 2.68322| Best val loss: 2.44223 | Time:  0.3860 hrs\n",
      "Step 3500 | Val Loss: 2.70687| Best val loss: 2.44223 | Time:  0.4533 hrs\n",
      "Step 4000 | Val Loss: 2.57962| Best val loss: 2.44223 | Time:  0.5202 hrs\n",
      "Step 4500 | Val Loss: 2.58411| Best val loss: 2.44223 | Time:  0.5880 hrs\n",
      "Step 5000 | Val Loss: 2.59728| Best val loss: 2.44223 | Time:  0.6560 hrs\n",
      "Step 5500 | Val Loss: 2.60684| Best val loss: 2.44223 | Time:  0.7256 hrs\n",
      "Step 6000 | Val Loss: 2.59413| Best val loss: 2.44223 | Time:  0.7969 hrs\n",
      "Step 6500 | Val Loss: 2.60678| Best val loss: 2.44223 | Time:  0.8659 hrs\n",
      "Step 7000 | Val Loss: 2.68473| Best val loss: 2.44223 | Time:  0.9367 hrs\n",
      "Step 7500 | Val Loss: 2.71318| Best val loss: 2.44223 | Time:  1.0068 hrs\n",
      "Step 8000 | Val Loss: 2.60787| Best val loss: 2.44223 | Time:  1.0770 hrs\n",
      "Step 8500 | Val Loss: 2.63369| Best val loss: 2.44223 | Time:  1.1488 hrs\n",
      "Step 9000 | Val Loss: 2.56639| Best val loss: 2.44223 | Time:  1.2218 hrs\n",
      "Step 9500 | Val Loss: 2.66069| Best val loss: 2.44223 | Time:  1.2914 hrs\n",
      "Step 10000 | Val Loss: 2.61006| Best val loss: 2.44223 | Time:  1.3636 hrs\n",
      "Step 10500 | Val Loss: 2.59780| Best val loss: 2.44223 | Time:  1.4360 hrs\n",
      "Step 11000 | Val Loss: 2.53158| Best val loss: 2.44223 | Time:  1.5087 hrs\n",
      "Step 11500 | Val Loss: 2.58687| Best val loss: 2.44223 | Time:  1.5821 hrs\n",
      "Step 12000 | Val Loss: 2.57381| Best val loss: 2.44223 | Time:  1.6556 hrs\n",
      "Step 12500 | Val Loss: 2.54659| Best val loss: 2.44223 | Time:  1.7276 hrs\n",
      "Step 13000 | Val Loss: 2.61579| Best val loss: 2.44223 | Time:  1.8006 hrs\n",
      "Step 13500 | Val Loss: 2.54021| Best val loss: 2.44223 | Time:  1.8745 hrs\n",
      "Step 14000 | Val Loss: 2.61393| Best val loss: 2.44223 | Time:  1.9467 hrs\n",
      "Step 14500 | Val Loss: 2.52075| Best val loss: 2.44223 | Time:  2.0212 hrs\n",
      "Step 15000 | Val Loss: 2.57397| Best val loss: 2.44223 | Time:  2.0935 hrs\n",
      "Step 15500 | Val Loss: 2.51164| Best val loss: 2.44223 | Time:  2.1674 hrs\n",
      "Step 16000 | Val Loss: 2.50589| Best val loss: 2.44223 | Time:  2.2427 hrs\n",
      "Step 16500 | Val Loss: 2.58622| Best val loss: 2.44223 | Time:  2.3154 hrs\n",
      "Step 17000 | Val Loss: 2.50172| Best val loss: 2.44223 | Time:  2.3877 hrs\n",
      "Step 17500 | Val Loss: 2.53977| Best val loss: 2.44223 | Time:  2.4675 hrs\n",
      "Step 18000 | Val Loss: 2.52348| Best val loss: 2.44223 | Time:  2.5491 hrs\n",
      "Step 18500 | Val Loss: 2.51620| Best val loss: 2.44223 | Time:  2.6318 hrs\n",
      "Step 19000 | Val Loss: 2.53400| Best val loss: 2.44223 | Time:  2.7161 hrs\n",
      "Step 19500 | Val Loss: 2.51631| Best val loss: 2.44223 | Time:  2.7981 hrs\n",
      "Step 20000 | Val Loss: 2.43568| Best val loss: 2.43568 | Time:  2.8818 hrs\n",
      "Step 20500 | Val Loss: 2.47333| Best val loss: 2.43568 | Time:  2.9658 hrs\n",
      "Step 21000 | Val Loss: 2.49924| Best val loss: 2.43568 | Time:  3.0489 hrs\n",
      "Step 21500 | Val Loss: 2.46323| Best val loss: 2.43568 | Time:  3.1305 hrs\n",
      "Step 22000 | Val Loss: 2.42545| Best val loss: 2.42545 | Time:  3.2144 hrs\n",
      "Step 22500 | Val Loss: 2.51686| Best val loss: 2.42545 | Time:  3.2991 hrs\n",
      "Step 23000 | Val Loss: 2.41755| Best val loss: 2.41755 | Time:  3.3845 hrs\n",
      "Step 23500 | Val Loss: 2.39066| Best val loss: 2.39066 | Time:  3.4937 hrs\n",
      "Step 24000 | Val Loss: 2.43307| Best val loss: 2.39066 | Time:  3.6158 hrs\n",
      "Step 24500 | Val Loss: 2.43501| Best val loss: 2.39066 | Time:  3.7395 hrs\n",
      "Step 25000 | Val Loss: 2.42222| Best val loss: 2.39066 | Time:  3.8634 hrs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation():\n",
    "    total_loss = 0\n",
    "    for i, (src, tgt) in enumerate(val_loader):\n",
    "        max_src_len = min(token_limit, max([len(s) for s in src])) + 1   #need this to accomodate max_len = 1\n",
    "        max_tgt_len = min(token_limit, max([len(s) for s in tgt])) + 1\n",
    "        inputs = tokenizer(src, padding='max_length', truncation=True, max_length=max_src_len)\n",
    "        targets = tokenizer(tgt, padding='max_length', truncation=True, max_length=max_tgt_len)\n",
    "        input_ids, input_masks = inputs['input_ids'], inputs['attention_mask']\n",
    "        target_ids, target_masks = targets['input_ids'], targets['attention_mask']\n",
    "        for j in range(len(target_ids)):\n",
    "            target_ids[j].insert(0, 1)\n",
    "            target_masks[j].insert(0, 1)\n",
    "        input_ids, input_masks = torch.tensor(input_ids).to(device), torch.tensor(input_masks).to(device)\n",
    "        target_ids, target_masks = torch.tensor(target_ids).to(device), torch.tensor(target_masks).to(device)\n",
    "        loss = MT_model(input_ids, input_masks, target_ids, target_masks)\n",
    "        total_loss += loss\n",
    "    return total_loss / len(val_loader)\n",
    "        \n",
    "\n",
    "min_val_loss = 10000\n",
    "PATH = 'saved_models/finetune.pt'\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"------------------------EPOCH {epoch + 1}-------------------------------\")\n",
    "    t1 = time.time()\n",
    "    for i, (src, tgt) in enumerate(train_loader):\n",
    "        MT_model.zero_grad()\n",
    "        \n",
    "        max_src_len = min(token_limit, max([len(s) for s in src])) + 1   #need this to accomodate max_len = 1\n",
    "        max_tgt_len = min(token_limit, max([len(s) for s in tgt])) + 1\n",
    "        inputs = tokenizer(src, padding='max_length', truncation=True, max_length=max_src_len)\n",
    "        targets = tokenizer(tgt, padding='max_length', truncation=True, max_length=max_tgt_len)\n",
    "        input_ids, input_masks = inputs['input_ids'], inputs['attention_mask']\n",
    "        target_ids, target_masks = targets['input_ids'], targets['attention_mask']\n",
    "#         print(len(input_ids[0]))\n",
    "        for j in range(len(target_ids)):\n",
    "            target_ids[j].insert(0, 1)\n",
    "            target_masks[j].insert(0, 1)\n",
    "#         print(len(input_ids[0]))\n",
    "#         print(MT_model._model.config.max_position_embeddings)\n",
    "#         print(tgt[0], target_ids[0])\n",
    "        \n",
    "        input_ids, input_masks = torch.tensor(input_ids).to(device), torch.tensor(input_masks).to(device)\n",
    "        target_ids, target_masks = torch.tensor(target_ids).to(device), torch.tensor(target_masks).to(device)\n",
    "        loss = MT_model(input_ids, input_masks, target_ids, target_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1)%500 == 0:\n",
    "            t2 = time.time()\n",
    "            val_loss = validation()\n",
    "            if val_loss.item() < min_val_loss:\n",
    "                torch.save(MT_model.state_dict(), PATH)\n",
    "                min_val_loss = val_loss\n",
    "            print(f'Step {i+1} | Val Loss: {val_loss.item():.5f}| Best val loss: {min_val_loss:.5f} | Time: {(t2-t1)/3600 : .4f} hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "790ee5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MT_model.load_state_dict(torch.load('saved_models/finetune.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "356d66ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'आप अपने अध्ययन पर ध्यान देना चाहिए</s>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_translate(model, device, tokenizer, input_sent):\n",
    "    tok_output = tokenizer(input_sent)\n",
    "    input_ids = tok_output['input_ids']\n",
    "    input_mask = tok_output['attention_mask']\n",
    "    input_ids = torch.tensor(input_ids, device=device)\n",
    "    input_mask = torch.tensor(input_mask, device=device)\n",
    "    target_mask = torch.ones([1, 1], device=device)\n",
    "    past_key_values = model.encode(input_ids, input_mask)\n",
    "#     print(past_key_values[0][0].shape)\n",
    "    start = [1]\n",
    "    gen = []\n",
    "    curr_token = None\n",
    "    while curr_token != 1:\n",
    "        tgt = torch.tensor(start, device=device)    \n",
    "        logits, past_key_values = model.decode(tgt, input_mask, target_mask, past_key_values)\n",
    "#         print(past_key_values[0][0].shape)\n",
    "        logits = model.logSoftmax(logits.unsqueeze(0)).squeeze(0)\n",
    "        value, index = torch.max(logits, dim=1)\n",
    "        curr_token = index[0].item()\n",
    "        gen.append(curr_token)\n",
    "        start = [curr_token]\n",
    "#         print(curr_token, value.item())\n",
    "        target_mask = torch.cat([target_mask, torch.ones([1, 1], device=device)], dim=1)\n",
    "    output_sent = tokenizer.decode(gen)\n",
    "    return output_sent\n",
    "\n",
    "sent = ['You should focus on your studies']\n",
    "greedy_translate(MT_model, device, tokenizer, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ca54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 sentences processed.\n",
      "20 sentences processed.\n",
      "30 sentences processed.\n",
      "40 sentences processed.\n",
      "50 sentences processed.\n",
      "60 sentences processed.\n",
      "70 sentences processed.\n",
      "80 sentences processed.\n",
      "90 sentences processed.\n",
      "100 sentences processed.\n",
      "110 sentences processed.\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "candidates = []\n",
    "references = []\n",
    "for i, (src, tgt) in enumerate(test_loader):\n",
    "    references.append(tgt[0])\n",
    "    candidate = greedy_translate(MT_model, device, tokenizer, [src[0]])\n",
    "    candidates.append(candidate)\n",
    "    if (i+1)%10 == 0:\n",
    "        print(f'{i+1} sentences processed.')\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(candidates, [references])\n",
    "print('BLEU score = {bleu}')\n",
    "\n",
    "# ref_file = 'path/to/reference/translations.txt'\n",
    "# with open(ref_file, 'r') as f:\n",
    "#     refs = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# hyp_file = 'path/to/candidate/translations.txt'\n",
    "# with open(hyp_file, 'r') as f:\n",
    "#     hyps = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
    "# bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5901cec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

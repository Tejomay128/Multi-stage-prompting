{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89cb1e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=6)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from prep_data import get_eng_hi_dataset\n",
    "from transformers import GPT2LMHeadModel, MT5Tokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69bb47ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(nn.Module):\n",
    "    def __init__(self, model) -> None:\n",
    "        super(LLM, self).__init__()\n",
    "        self._model = model\n",
    "        self.hidden_size = model.config.hidden_size\n",
    "        self.embed_size = model.config.hidden_size\n",
    "        self.n_layers = model.config.num_hidden_layers\n",
    "        self.n_heads = model.config.num_attention_heads\n",
    "        self.head_size = self.embed_size // self.n_heads\n",
    "\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=2)\n",
    "        self.logSoftmax_1 = nn.LogSoftmax(dim=1)\n",
    "        self.nll = nn.NLLLoss()\n",
    "\n",
    "\n",
    "    def encode(self, input_ids, input_mask):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        len_sent = input_ids.shape[1]\n",
    "        attn_mask = input_mask\n",
    "\n",
    "        outputs = self._model(input_ids, \n",
    "                              attention_mask=attn_mask,  \n",
    "                              use_cache=True)\n",
    "\n",
    "        #RE-ENCODING\n",
    "        layer_prefix_list = []\n",
    "        attn_mask = torch.cat([input_mask, input_mask], dim=1)\n",
    "\n",
    "        outputs = self._model(input_ids, \n",
    "                              past_key_values=outputs.past_key_values, \n",
    "                              attention_mask=attn_mask, \n",
    "                              use_cache=True)\n",
    "        \n",
    "        past_key_values = []\n",
    "        for (key, value) in outputs.past_key_values:\n",
    "            k = key[:,:,len_sent:,:]\n",
    "            v = value[:,:,len_sent:,:]\n",
    "            past_key_values.append((k, v))\n",
    "\n",
    "        return past_key_values\n",
    "    \n",
    "    def decode(self, target_ids, input_mask, target_mask, past_key_values, mode='train'):\n",
    "        batch_size = target_ids.shape[0]\n",
    "        attn_mask = torch.cat([input_mask, target_mask], dim=1)\n",
    "\n",
    "        outputs = self._model(target_ids, \n",
    "                              past_key_values=past_key_values, \n",
    "                              attention_mask=attn_mask,  \n",
    "                              use_cache=True)\n",
    "        return outputs.logits, outputs.past_key_values\n",
    "    \n",
    "    def forward(self, input_ids, input_mask, target_ids, target_mask):\n",
    "        past_key_values = self.encode(input_ids, input_mask)\n",
    "        labels = target_ids[:, 1:]\n",
    "        target_ids = target_ids[:, :-1]\n",
    "        target_mask = target_mask[:, :-1]\n",
    "        logits,_ = self.decode(target_ids, input_mask, target_mask, past_key_values)\n",
    "\n",
    "        # make batch size and sentence length as one dimension\n",
    "        logprobs = self.logSoftmax(logits)\n",
    "        logprobs = logprobs.reshape([logprobs.shape[0] * logprobs.shape[1], -1])\n",
    "        target_mask = target_mask.reshape([target_mask.shape[0] * target_mask.shape[1],])\n",
    "        labels = labels.flatten()\n",
    "        loss = -logprobs[torch.arange(logprobs.shape[0], device=labels.device), labels]\n",
    "#         print(loss.shape, target_mask.shape)\n",
    "        loss = torch.sum(loss * target_mask) / torch.sum(target_mask)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cefe0230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/cfilt___parquet/cfilt--iitb-english-hindi-911387c6837f8b91/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129da9b38a3349b181a2c54461ba3e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_data(PATH):\n",
    "    dataset = []\n",
    "    f_en = open(PATH + 'filtered.en', 'r')\n",
    "    for line in f_en.readlines():\n",
    "        line = line.strip('\\n')\n",
    "        entry = {'en': line}\n",
    "        dataset.append(entry)\n",
    "    f_en.close()\n",
    "    \n",
    "    f_hi = open(PATH + 'filtered.hi', 'r')\n",
    "    for i, line in enumerate(f_hi.readlines()):\n",
    "        line = line.strip('\\n')\n",
    "        dataset[i]['hi'] = line\n",
    "    f_hi.close()\n",
    "    return dataset\n",
    "\n",
    "val_data, test_data = get_eng_hi_dataset()\n",
    "train_data = read_data('filtered_data/')\n",
    "train_data = train_data[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56f20c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelCorpus(Dataset):\n",
    "    def __init__(self, data, src_lang='en', tgt_lang='hi') -> None:\n",
    "        super(ParallelCorpus, self).__init__()\n",
    "        self.src = []\n",
    "        self.tgt = []\n",
    "        for pair in data:\n",
    "            self.src.append(pair[src_lang])\n",
    "            self.tgt.append(pair[tgt_lang])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.tgt[index]\n",
    "\n",
    "train_pc = ParallelCorpus(train_data, src_lang='en', tgt_lang='hi')\n",
    "test_pc = ParallelCorpus(test_data, src_lang='en', tgt_lang='hi')\n",
    "val_pc = ParallelCorpus(val_data, src_lang='en', tgt_lang='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a0f8fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_prefix = 100\n",
    "lr = 1e-4\n",
    "batch_size = 4\n",
    "num_epochs = 2\n",
    "token_limit = ((1023 - len_prefix) // 2) - 3  #to accomodate extra one token if max_len=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7688d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_pc, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_pc, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_pc, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8061529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'T5Tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MT5Tokenizer.from_pretrained(\"THUMT/mGPT\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"THUMT/mGPT\")\n",
    "\n",
    "MT_model = LLM(model).to(device)\n",
    "optimizer = torch.optim.Adam(params=MT_model.parameters(),lr=lr, eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b155af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------EPOCH 1-------------------------------\n",
      "Step 500 | Val Loss: 3.35807| Best val loss: 3.35807 | Time:  0.0575 hrs\n",
      "Step 1000 | Val Loss: 3.44138| Best val loss: 3.35807 | Time:  0.1227 hrs\n",
      "Step 1500 | Val Loss: 3.28490| Best val loss: 3.28490 | Time:  0.1880 hrs\n",
      "Step 2000 | Val Loss: 3.36685| Best val loss: 3.28490 | Time:  0.2560 hrs\n",
      "Step 2500 | Val Loss: 3.20250| Best val loss: 3.20250 | Time:  0.3220 hrs\n",
      "Step 3000 | Val Loss: 3.32183| Best val loss: 3.20250 | Time:  0.3891 hrs\n",
      "Step 3500 | Val Loss: 3.18532| Best val loss: 3.18532 | Time:  0.4561 hrs\n",
      "Step 4000 | Val Loss: 3.06976| Best val loss: 3.06976 | Time:  0.5239 hrs\n",
      "Step 4500 | Val Loss: 2.97176| Best val loss: 2.97176 | Time:  0.5931 hrs\n",
      "Step 5000 | Val Loss: 3.05036| Best val loss: 2.97176 | Time:  0.6623 hrs\n",
      "Step 5500 | Val Loss: 2.95531| Best val loss: 2.95531 | Time:  0.7316 hrs\n",
      "Step 6000 | Val Loss: 2.93756| Best val loss: 2.93756 | Time:  0.8049 hrs\n",
      "Step 6500 | Val Loss: 2.93002| Best val loss: 2.93002 | Time:  0.8751 hrs\n",
      "Step 7000 | Val Loss: 2.95588| Best val loss: 2.93002 | Time:  0.9468 hrs\n",
      "Step 7500 | Val Loss: 2.94694| Best val loss: 2.93002 | Time:  1.0160 hrs\n",
      "Step 8000 | Val Loss: 2.94861| Best val loss: 2.93002 | Time:  1.0855 hrs\n",
      "Step 8500 | Val Loss: 2.94061| Best val loss: 2.93002 | Time:  1.1571 hrs\n",
      "Step 9000 | Val Loss: 2.79523| Best val loss: 2.79523 | Time:  1.2339 hrs\n",
      "Step 9500 | Val Loss: 2.87109| Best val loss: 2.79523 | Time:  1.3040 hrs\n",
      "Step 10000 | Val Loss: 2.92027| Best val loss: 2.79523 | Time:  1.3750 hrs\n",
      "Step 10500 | Val Loss: 2.81411| Best val loss: 2.79523 | Time:  1.4457 hrs\n",
      "Step 11000 | Val Loss: 2.77896| Best val loss: 2.77896 | Time:  1.5169 hrs\n",
      "Step 11500 | Val Loss: 2.78643| Best val loss: 2.77896 | Time:  1.5901 hrs\n",
      "Step 12000 | Val Loss: 2.75958| Best val loss: 2.75958 | Time:  1.6618 hrs\n",
      "Step 12500 | Val Loss: 2.74054| Best val loss: 2.74054 | Time:  1.7358 hrs\n",
      "Step 13000 | Val Loss: 2.73694| Best val loss: 2.73694 | Time:  1.8105 hrs\n",
      "Step 13500 | Val Loss: 2.78642| Best val loss: 2.73694 | Time:  1.8871 hrs\n",
      "Step 14000 | Val Loss: 2.74195| Best val loss: 2.73694 | Time:  1.9603 hrs\n",
      "Step 14500 | Val Loss: 2.72402| Best val loss: 2.72402 | Time:  2.0355 hrs\n",
      "Step 15000 | Val Loss: 2.73151| Best val loss: 2.72402 | Time:  2.1099 hrs\n",
      "Step 15500 | Val Loss: 2.70830| Best val loss: 2.70830 | Time:  2.1848 hrs\n",
      "Step 16000 | Val Loss: 2.61097| Best val loss: 2.61097 | Time:  2.2617 hrs\n",
      "Step 16500 | Val Loss: 2.64306| Best val loss: 2.61097 | Time:  2.3341 hrs\n",
      "Step 17000 | Val Loss: 2.67702| Best val loss: 2.61097 | Time:  2.4047 hrs\n",
      "Step 17500 | Val Loss: 2.65603| Best val loss: 2.61097 | Time:  2.4779 hrs\n",
      "Step 18000 | Val Loss: 2.67725| Best val loss: 2.61097 | Time:  2.5551 hrs\n",
      "Step 18500 | Val Loss: 2.60041| Best val loss: 2.60041 | Time:  2.6288 hrs\n",
      "Step 19000 | Val Loss: 2.59733| Best val loss: 2.59733 | Time:  2.7048 hrs\n",
      "Step 19500 | Val Loss: 2.61281| Best val loss: 2.59733 | Time:  2.7793 hrs\n",
      "Step 20000 | Val Loss: 2.53379| Best val loss: 2.53379 | Time:  2.8541 hrs\n",
      "Step 20500 | Val Loss: 2.58165| Best val loss: 2.53379 | Time:  2.9293 hrs\n",
      "Step 21000 | Val Loss: 2.54590| Best val loss: 2.53379 | Time:  3.0044 hrs\n",
      "Step 21500 | Val Loss: 2.59035| Best val loss: 2.53379 | Time:  3.0782 hrs\n",
      "Step 22000 | Val Loss: 2.50845| Best val loss: 2.50845 | Time:  3.1532 hrs\n",
      "Step 22500 | Val Loss: 2.58219| Best val loss: 2.50845 | Time:  3.2303 hrs\n",
      "Step 23000 | Val Loss: 2.44223| Best val loss: 2.44223 | Time:  3.3067 hrs\n",
      "Step 23500 | Val Loss: 2.48382| Best val loss: 2.44223 | Time:  3.3855 hrs\n",
      "Step 24000 | Val Loss: 2.44835| Best val loss: 2.44223 | Time:  3.4612 hrs\n",
      "Step 24500 | Val Loss: 2.45489| Best val loss: 2.44223 | Time:  3.5372 hrs\n",
      "Step 25000 | Val Loss: 2.50830| Best val loss: 2.44223 | Time:  3.6120 hrs\n",
      "------------------------EPOCH 2-------------------------------\n",
      "Step 500 | Val Loss: 2.62803| Best val loss: 2.44223 | Time:  0.0573 hrs\n",
      "Step 1000 | Val Loss: 2.62492| Best val loss: 2.44223 | Time:  0.1210 hrs\n",
      "Step 1500 | Val Loss: 2.63288| Best val loss: 2.44223 | Time:  0.1860 hrs\n",
      "Step 2000 | Val Loss: 2.73399| Best val loss: 2.44223 | Time:  0.2530 hrs\n",
      "Step 2500 | Val Loss: 2.65545| Best val loss: 2.44223 | Time:  0.3195 hrs\n",
      "Step 3000 | Val Loss: 2.68322| Best val loss: 2.44223 | Time:  0.3860 hrs\n",
      "Step 3500 | Val Loss: 2.70687| Best val loss: 2.44223 | Time:  0.4533 hrs\n",
      "Step 4000 | Val Loss: 2.57962| Best val loss: 2.44223 | Time:  0.5202 hrs\n",
      "Step 4500 | Val Loss: 2.58411| Best val loss: 2.44223 | Time:  0.5880 hrs\n",
      "Step 5000 | Val Loss: 2.59728| Best val loss: 2.44223 | Time:  0.6560 hrs\n",
      "Step 5500 | Val Loss: 2.60684| Best val loss: 2.44223 | Time:  0.7256 hrs\n",
      "Step 6000 | Val Loss: 2.59413| Best val loss: 2.44223 | Time:  0.7969 hrs\n",
      "Step 6500 | Val Loss: 2.60678| Best val loss: 2.44223 | Time:  0.8659 hrs\n",
      "Step 7000 | Val Loss: 2.68473| Best val loss: 2.44223 | Time:  0.9367 hrs\n",
      "Step 7500 | Val Loss: 2.71318| Best val loss: 2.44223 | Time:  1.0068 hrs\n",
      "Step 8000 | Val Loss: 2.60787| Best val loss: 2.44223 | Time:  1.0770 hrs\n",
      "Step 8500 | Val Loss: 2.63369| Best val loss: 2.44223 | Time:  1.1488 hrs\n",
      "Step 9000 | Val Loss: 2.56639| Best val loss: 2.44223 | Time:  1.2218 hrs\n",
      "Step 9500 | Val Loss: 2.66069| Best val loss: 2.44223 | Time:  1.2914 hrs\n",
      "Step 10000 | Val Loss: 2.61006| Best val loss: 2.44223 | Time:  1.3636 hrs\n",
      "Step 10500 | Val Loss: 2.59780| Best val loss: 2.44223 | Time:  1.4360 hrs\n",
      "Step 11000 | Val Loss: 2.53158| Best val loss: 2.44223 | Time:  1.5087 hrs\n",
      "Step 11500 | Val Loss: 2.58687| Best val loss: 2.44223 | Time:  1.5821 hrs\n",
      "Step 12000 | Val Loss: 2.57381| Best val loss: 2.44223 | Time:  1.6556 hrs\n",
      "Step 12500 | Val Loss: 2.54659| Best val loss: 2.44223 | Time:  1.7276 hrs\n",
      "Step 13000 | Val Loss: 2.61579| Best val loss: 2.44223 | Time:  1.8006 hrs\n",
      "Step 13500 | Val Loss: 2.54021| Best val loss: 2.44223 | Time:  1.8745 hrs\n",
      "Step 14000 | Val Loss: 2.61393| Best val loss: 2.44223 | Time:  1.9467 hrs\n",
      "Step 14500 | Val Loss: 2.52075| Best val loss: 2.44223 | Time:  2.0212 hrs\n",
      "Step 15000 | Val Loss: 2.57397| Best val loss: 2.44223 | Time:  2.0935 hrs\n",
      "Step 15500 | Val Loss: 2.51164| Best val loss: 2.44223 | Time:  2.1674 hrs\n",
      "Step 16000 | Val Loss: 2.50589| Best val loss: 2.44223 | Time:  2.2427 hrs\n",
      "Step 16500 | Val Loss: 2.58622| Best val loss: 2.44223 | Time:  2.3154 hrs\n",
      "Step 17000 | Val Loss: 2.50172| Best val loss: 2.44223 | Time:  2.3877 hrs\n",
      "Step 17500 | Val Loss: 2.53977| Best val loss: 2.44223 | Time:  2.4675 hrs\n",
      "Step 18000 | Val Loss: 2.52348| Best val loss: 2.44223 | Time:  2.5491 hrs\n",
      "Step 18500 | Val Loss: 2.51620| Best val loss: 2.44223 | Time:  2.6318 hrs\n",
      "Step 19000 | Val Loss: 2.53400| Best val loss: 2.44223 | Time:  2.7161 hrs\n",
      "Step 19500 | Val Loss: 2.51631| Best val loss: 2.44223 | Time:  2.7981 hrs\n",
      "Step 20000 | Val Loss: 2.43568| Best val loss: 2.43568 | Time:  2.8818 hrs\n",
      "Step 20500 | Val Loss: 2.47333| Best val loss: 2.43568 | Time:  2.9658 hrs\n",
      "Step 21000 | Val Loss: 2.49924| Best val loss: 2.43568 | Time:  3.0489 hrs\n",
      "Step 21500 | Val Loss: 2.46323| Best val loss: 2.43568 | Time:  3.1305 hrs\n",
      "Step 22000 | Val Loss: 2.42545| Best val loss: 2.42545 | Time:  3.2144 hrs\n",
      "Step 22500 | Val Loss: 2.51686| Best val loss: 2.42545 | Time:  3.2991 hrs\n",
      "Step 23000 | Val Loss: 2.41755| Best val loss: 2.41755 | Time:  3.3845 hrs\n",
      "Step 23500 | Val Loss: 2.39066| Best val loss: 2.39066 | Time:  3.4937 hrs\n",
      "Step 24000 | Val Loss: 2.43307| Best val loss: 2.39066 | Time:  3.6158 hrs\n",
      "Step 24500 | Val Loss: 2.43501| Best val loss: 2.39066 | Time:  3.7395 hrs\n",
      "Step 25000 | Val Loss: 2.42222| Best val loss: 2.39066 | Time:  3.8634 hrs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation():\n",
    "    total_loss = 0\n",
    "    for i, (src, tgt) in enumerate(val_loader):\n",
    "        max_src_len = min(token_limit, max([len(s) for s in src])) + 1   #need this to accomodate max_len = 1\n",
    "        max_tgt_len = min(token_limit, max([len(s) for s in tgt])) + 1\n",
    "        inputs = tokenizer(src, padding='max_length', truncation=True, max_length=max_src_len)\n",
    "        targets = tokenizer(tgt, padding='max_length', truncation=True, max_length=max_tgt_len)\n",
    "        input_ids, input_masks = inputs['input_ids'], inputs['attention_mask']\n",
    "        target_ids, target_masks = targets['input_ids'], targets['attention_mask']\n",
    "        for j in range(len(target_ids)):\n",
    "            target_ids[j].insert(0, 1)\n",
    "            target_masks[j].insert(0, 1)\n",
    "        input_ids, input_masks = torch.tensor(input_ids).to(device), torch.tensor(input_masks).to(device)\n",
    "        target_ids, target_masks = torch.tensor(target_ids).to(device), torch.tensor(target_masks).to(device)\n",
    "        loss = MT_model(input_ids, input_masks, target_ids, target_masks)\n",
    "        total_loss += loss\n",
    "    return total_loss / len(val_loader)\n",
    "        \n",
    "\n",
    "min_val_loss = 10000\n",
    "PATH = 'saved_models/finetune.pt'\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"------------------------EPOCH {epoch + 1}-------------------------------\")\n",
    "    t1 = time.time()\n",
    "    for i, (src, tgt) in enumerate(train_loader):\n",
    "        MT_model.zero_grad()\n",
    "        \n",
    "        max_src_len = min(token_limit, max([len(s) for s in src])) + 1   #need this to accomodate max_len = 1\n",
    "        max_tgt_len = min(token_limit, max([len(s) for s in tgt])) + 1\n",
    "        inputs = tokenizer(src, padding='max_length', truncation=True, max_length=max_src_len)\n",
    "        targets = tokenizer(tgt, padding='max_length', truncation=True, max_length=max_tgt_len)\n",
    "        input_ids, input_masks = inputs['input_ids'], inputs['attention_mask']\n",
    "        target_ids, target_masks = targets['input_ids'], targets['attention_mask']\n",
    "#         print(len(input_ids[0]))\n",
    "        for j in range(len(target_ids)):\n",
    "            target_ids[j].insert(0, 1)\n",
    "            target_masks[j].insert(0, 1)\n",
    "#         print(len(input_ids[0]))\n",
    "#         print(MT_model._model.config.max_position_embeddings)\n",
    "#         print(tgt[0], target_ids[0])\n",
    "        \n",
    "        input_ids, input_masks = torch.tensor(input_ids).to(device), torch.tensor(input_masks).to(device)\n",
    "        target_ids, target_masks = torch.tensor(target_ids).to(device), torch.tensor(target_masks).to(device)\n",
    "        loss = MT_model(input_ids, input_masks, target_ids, target_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1)%500 == 0:\n",
    "            t2 = time.time()\n",
    "            val_loss = validation()\n",
    "            if val_loss.item() < min_val_loss:\n",
    "                torch.save(MT_model.state_dict(), PATH)\n",
    "                min_val_loss = val_loss\n",
    "            print(f'Step {i+1} | Val Loss: {val_loss.item():.5f}| Best val loss: {min_val_loss:.5f} | Time: {(t2-t1)/3600 : .4f} hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df186c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MT_model.load_state_dict(torch.load('saved_models/finetune.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab48bd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'मैं स्कूल जा रही हूँ।</s>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_translate(model, device, tokenizer, input_sent):\n",
    "    tok_output = tokenizer(input_sent, truncation=True, max_length=token_limit)\n",
    "    input_ids = tok_output['input_ids']\n",
    "    input_mask = tok_output['attention_mask']\n",
    "    input_ids = torch.tensor(input_ids, device=device)\n",
    "    input_mask = torch.tensor(input_mask, device=device)\n",
    "    target_mask = torch.ones([1, 1], device=device)\n",
    "    past_key_values = model.encode(input_ids, input_mask)\n",
    "#     print(past_key_values[0][0].shape)\n",
    "    start = [1]\n",
    "    gen = []\n",
    "    curr_token = None\n",
    "    while curr_token != 1 and len(gen) < token_limit:\n",
    "        tgt = torch.tensor(start, device=device)    \n",
    "        logits, past_key_values = model.decode(tgt, input_mask, target_mask, past_key_values)\n",
    "#         print(past_key_values[0][0].shape)\n",
    "        logits = model.logSoftmax(logits.unsqueeze(0)).squeeze(0)\n",
    "        value, index = torch.max(logits, dim=1)\n",
    "        curr_token = index[0].item()\n",
    "        gen.append(curr_token)\n",
    "        start = [curr_token]\n",
    "#         print(curr_token, value.item())\n",
    "        target_mask = torch.cat([target_mask, torch.ones([1, 1], device=device)], dim=1)\n",
    "    output_sent = tokenizer.decode(gen)\n",
    "    return output_sent\n",
    "\n",
    "sent = ['I am going to school.']\n",
    "greedy_translate(MT_model, device, tokenizer, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7249637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 sentences processed.\n",
      "20 sentences processed.\n",
      "30 sentences processed.\n",
      "40 sentences processed.\n",
      "50 sentences processed.\n",
      "60 sentences processed.\n",
      "70 sentences processed.\n",
      "80 sentences processed.\n",
      "90 sentences processed.\n",
      "100 sentences processed.\n",
      "110 sentences processed.\n",
      "120 sentences processed.\n",
      "130 sentences processed.\n",
      "140 sentences processed.\n",
      "150 sentences processed.\n",
      "160 sentences processed.\n",
      "170 sentences processed.\n",
      "180 sentences processed.\n",
      "190 sentences processed.\n",
      "200 sentences processed.\n",
      "210 sentences processed.\n",
      "220 sentences processed.\n",
      "230 sentences processed.\n",
      "240 sentences processed.\n",
      "250 sentences processed.\n",
      "260 sentences processed.\n",
      "270 sentences processed.\n",
      "280 sentences processed.\n",
      "290 sentences processed.\n",
      "300 sentences processed.\n",
      "310 sentences processed.\n",
      "320 sentences processed.\n",
      "330 sentences processed.\n",
      "340 sentences processed.\n",
      "350 sentences processed.\n",
      "360 sentences processed.\n",
      "370 sentences processed.\n",
      "380 sentences processed.\n",
      "390 sentences processed.\n",
      "400 sentences processed.\n",
      "410 sentences processed.\n",
      "420 sentences processed.\n",
      "430 sentences processed.\n",
      "440 sentences processed.\n",
      "450 sentences processed.\n",
      "460 sentences processed.\n",
      "470 sentences processed.\n",
      "480 sentences processed.\n",
      "490 sentences processed.\n",
      "500 sentences processed.\n",
      "510 sentences processed.\n",
      "520 sentences processed.\n",
      "530 sentences processed.\n",
      "540 sentences processed.\n",
      "550 sentences processed.\n",
      "560 sentences processed.\n",
      "570 sentences processed.\n",
      "580 sentences processed.\n",
      "590 sentences processed.\n",
      "600 sentences processed.\n",
      "610 sentences processed.\n",
      "620 sentences processed.\n",
      "630 sentences processed.\n",
      "640 sentences processed.\n",
      "650 sentences processed.\n",
      "660 sentences processed.\n",
      "670 sentences processed.\n",
      "680 sentences processed.\n",
      "690 sentences processed.\n",
      "700 sentences processed.\n",
      "710 sentences processed.\n",
      "720 sentences processed.\n",
      "730 sentences processed.\n",
      "740 sentences processed.\n",
      "750 sentences processed.\n",
      "760 sentences processed.\n",
      "770 sentences processed.\n",
      "780 sentences processed.\n",
      "790 sentences processed.\n",
      "800 sentences processed.\n",
      "810 sentences processed.\n",
      "820 sentences processed.\n",
      "830 sentences processed.\n",
      "840 sentences processed.\n",
      "850 sentences processed.\n",
      "860 sentences processed.\n",
      "870 sentences processed.\n",
      "880 sentences processed.\n",
      "890 sentences processed.\n",
      "900 sentences processed.\n",
      "910 sentences processed.\n",
      "920 sentences processed.\n",
      "930 sentences processed.\n",
      "940 sentences processed.\n",
      "950 sentences processed.\n",
      "960 sentences processed.\n",
      "970 sentences processed.\n",
      "980 sentences processed.\n",
      "990 sentences processed.\n",
      "1000 sentences processed.\n",
      "1010 sentences processed.\n",
      "1020 sentences processed.\n",
      "1030 sentences processed.\n",
      "1040 sentences processed.\n",
      "1050 sentences processed.\n",
      "1060 sentences processed.\n",
      "1070 sentences processed.\n",
      "1080 sentences processed.\n",
      "1090 sentences processed.\n",
      "1100 sentences processed.\n",
      "1110 sentences processed.\n",
      "1120 sentences processed.\n",
      "1130 sentences processed.\n",
      "1140 sentences processed.\n",
      "1150 sentences processed.\n",
      "1160 sentences processed.\n",
      "1170 sentences processed.\n",
      "1180 sentences processed.\n",
      "1190 sentences processed.\n",
      "1200 sentences processed.\n",
      "1210 sentences processed.\n",
      "1220 sentences processed.\n",
      "1230 sentences processed.\n",
      "1240 sentences processed.\n",
      "1250 sentences processed.\n",
      "1260 sentences processed.\n",
      "1270 sentences processed.\n",
      "1280 sentences processed.\n",
      "1290 sentences processed.\n",
      "1300 sentences processed.\n",
      "1310 sentences processed.\n",
      "1320 sentences processed.\n",
      "1330 sentences processed.\n",
      "1340 sentences processed.\n",
      "1350 sentences processed.\n",
      "1360 sentences processed.\n",
      "1370 sentences processed.\n",
      "1380 sentences processed.\n",
      "1390 sentences processed.\n",
      "1400 sentences processed.\n",
      "1410 sentences processed.\n",
      "1420 sentences processed.\n",
      "1430 sentences processed.\n",
      "1440 sentences processed.\n",
      "1450 sentences processed.\n",
      "1460 sentences processed.\n",
      "1470 sentences processed.\n",
      "1480 sentences processed.\n",
      "1490 sentences processed.\n",
      "1500 sentences processed.\n",
      "1510 sentences processed.\n",
      "1520 sentences processed.\n",
      "1530 sentences processed.\n",
      "1540 sentences processed.\n",
      "1550 sentences processed.\n",
      "1560 sentences processed.\n",
      "1570 sentences processed.\n",
      "1580 sentences processed.\n",
      "1590 sentences processed.\n",
      "1600 sentences processed.\n",
      "1610 sentences processed.\n",
      "1620 sentences processed.\n",
      "1630 sentences processed.\n",
      "1640 sentences processed.\n",
      "1650 sentences processed.\n",
      "1660 sentences processed.\n",
      "1670 sentences processed.\n",
      "1680 sentences processed.\n",
      "1690 sentences processed.\n",
      "1700 sentences processed.\n",
      "1710 sentences processed.\n",
      "1720 sentences processed.\n",
      "1730 sentences processed.\n",
      "1740 sentences processed.\n",
      "1750 sentences processed.\n",
      "1760 sentences processed.\n",
      "1770 sentences processed.\n",
      "1780 sentences processed.\n",
      "1790 sentences processed.\n",
      "1800 sentences processed.\n",
      "1810 sentences processed.\n",
      "1820 sentences processed.\n",
      "1830 sentences processed.\n",
      "1840 sentences processed.\n",
      "1850 sentences processed.\n",
      "1860 sentences processed.\n",
      "1870 sentences processed.\n",
      "1880 sentences processed.\n",
      "1890 sentences processed.\n",
      "1900 sentences processed.\n",
      "1910 sentences processed.\n",
      "1920 sentences processed.\n",
      "1930 sentences processed.\n",
      "1940 sentences processed.\n",
      "1950 sentences processed.\n",
      "1960 sentences processed.\n",
      "1970 sentences processed.\n",
      "1980 sentences processed.\n",
      "1990 sentences processed.\n",
      "2000 sentences processed.\n",
      "2010 sentences processed.\n",
      "2020 sentences processed.\n",
      "2030 sentences processed.\n",
      "2040 sentences processed.\n",
      "2050 sentences processed.\n",
      "2060 sentences processed.\n",
      "2070 sentences processed.\n",
      "2080 sentences processed.\n",
      "2090 sentences processed.\n",
      "2100 sentences processed.\n",
      "2110 sentences processed.\n",
      "2120 sentences processed.\n",
      "2130 sentences processed.\n",
      "2140 sentences processed.\n",
      "2150 sentences processed.\n",
      "2160 sentences processed.\n",
      "2170 sentences processed.\n",
      "2180 sentences processed.\n",
      "2190 sentences processed.\n",
      "2200 sentences processed.\n",
      "2210 sentences processed.\n",
      "2220 sentences processed.\n",
      "2230 sentences processed.\n",
      "2240 sentences processed.\n",
      "2250 sentences processed.\n",
      "2260 sentences processed.\n",
      "2270 sentences processed.\n",
      "2280 sentences processed.\n",
      "2290 sentences processed.\n",
      "2300 sentences processed.\n",
      "2310 sentences processed.\n",
      "2320 sentences processed.\n",
      "2330 sentences processed.\n",
      "2340 sentences processed.\n",
      "2350 sentences processed.\n",
      "2360 sentences processed.\n",
      "2370 sentences processed.\n",
      "2380 sentences processed.\n",
      "2390 sentences processed.\n",
      "2400 sentences processed.\n",
      "2410 sentences processed.\n",
      "2420 sentences processed.\n",
      "2430 sentences processed.\n",
      "2440 sentences processed.\n",
      "2450 sentences processed.\n",
      "2460 sentences processed.\n",
      "2470 sentences processed.\n",
      "2480 sentences processed.\n",
      "2490 sentences processed.\n",
      "2500 sentences processed.\n",
      "BLEU score = {bleu}\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "candidates = []\n",
    "references = []\n",
    "for i, (src, tgt) in enumerate(test_loader):\n",
    "    references.append(tgt[0])\n",
    "    candidate = greedy_translate(MT_model, device, tokenizer, [src[0]])\n",
    "    candidates.append(candidate[:-4])\n",
    "    if (i+1)%10 == 0:\n",
    "        print(f'{i+1} sentences processed.')\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(candidates, [references])\n",
    "print(f'BLEU score = {bleu}')\n",
    "\n",
    "# ref_file = 'path/to/reference/translations.txt'\n",
    "# with open(ref_file, 'r') as f:\n",
    "#     refs = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# hyp_file = 'path/to/candidate/translations.txt'\n",
    "# with open(hyp_file, 'r') as f:\n",
    "#     hyps = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
    "# bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4d8a02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 6.90 33.1/11.1/4.1/1.5 (BP = 0.991 ratio = 0.991 hyp_len = 60289 ref_len = 60821)\n",
      "chrF2++ = 35.38\n"
     ]
    }
   ],
   "source": [
    "chrF3 = sacrebleu.corpus_chrf(candidates, [references], char_order=3, word_order=2)\n",
    "print(f'{bleu}\\n{chrF3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d741e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

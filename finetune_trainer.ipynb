{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0dc383d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from prep_data import get_eng_hi_dataset\n",
    "from transformers import GPT2LMHeadModel, MT5Tokenizer\n",
    "\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a204e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(nn.Module):\n",
    "    def __init__(self, model) -> None:\n",
    "        super(LLM, self).__init__()\n",
    "        self._model = model\n",
    "        self.hidden_size = model.config.hidden_size\n",
    "        self.embed_size = model.config.hidden_size\n",
    "        self.n_layers = model.config.num_hidden_layers\n",
    "        self.n_heads = model.config.num_attention_heads\n",
    "        self.head_size = self.embed_size // self.n_heads\n",
    "\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=2)\n",
    "        self.logSoftmax_1 = nn.LogSoftmax(dim=1)\n",
    "        self.nll = nn.NLLLoss()\n",
    "\n",
    "\n",
    "    def encode(self, input_ids, input_mask):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        len_sent = input_ids.shape[1]\n",
    "        attn_mask = input_mask\n",
    "\n",
    "        outputs = self._model(input_ids, \n",
    "                              attention_mask=attn_mask,  \n",
    "                              use_cache=True)\n",
    "\n",
    "        #RE-ENCODING\n",
    "        layer_prefix_list = []\n",
    "        attn_mask = torch.cat([input_mask, input_mask], dim=1)\n",
    "\n",
    "        outputs = self._model(input_ids, \n",
    "                              past_key_values=outputs.past_key_values, \n",
    "                              attention_mask=attn_mask, \n",
    "                              use_cache=True)\n",
    "        \n",
    "        past_key_values = []\n",
    "        for (key, value) in outputs.past_key_values:\n",
    "            k = key[:,:,len_sent:,:]\n",
    "            v = value[:,:,len_sent:,:]\n",
    "            past_key_values.append((k, v))\n",
    "\n",
    "        return past_key_values\n",
    "    \n",
    "    def decode(self, target_ids, input_mask, target_mask, past_key_values, mode='train'):\n",
    "        batch_size = target_ids.shape[0]\n",
    "        attn_mask = torch.cat([input_mask, target_mask], dim=1)\n",
    "\n",
    "        outputs = self._model(target_ids, \n",
    "                              past_key_values=past_key_values, \n",
    "                              attention_mask=attn_mask,  \n",
    "                              use_cache=True)\n",
    "        return outputs.logits, outputs.past_key_values\n",
    "    \n",
    "    def forward(self, input_ids, input_mask, target_ids, target_mask):\n",
    "        past_key_values = self.encode(input_ids, input_mask)\n",
    "        labels = target_ids[:, 1:]\n",
    "        target_ids = target_ids[:, :-1]\n",
    "        target_mask = target_mask[:, :-1]\n",
    "        logits,_ = self.decode(target_ids, input_mask, target_mask, past_key_values)\n",
    "\n",
    "        # make batch size and sentence length as one dimension\n",
    "        logprobs = self.logSoftmax(logits)\n",
    "        logprobs = logprobs.reshape([logprobs.shape[0] * logprobs.shape[1], -1])\n",
    "        target_mask = target_mask.reshape([target_mask.shape[0] * target_mask.shape[1],])\n",
    "        labels = labels.flatten()\n",
    "        loss = -logprobs[torch.arange(logprobs.shape[0], device=labels.device), labels]\n",
    "#         print(loss.shape, target_mask.shape)\n",
    "        loss = torch.sum(loss * target_mask) / torch.sum(target_mask)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e5b6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/cfilt___parquet/cfilt--iitb-english-hindi-911387c6837f8b91/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02bfff83f98244e9a7b28bf2f02594df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data, val_data, test_data = get_eng_hi_dataset()\n",
    "train_data = train_data[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f4d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelCorpus(Dataset):\n",
    "    def __init__(self, data, src_lang='en', tgt_lang='hi') -> None:\n",
    "        super(ParallelCorpus, self).__init__()\n",
    "        self.src = []\n",
    "        self.tgt = []\n",
    "        for pair in data:\n",
    "            self.src.append(pair[src_lang])\n",
    "            self.tgt.append(pair[tgt_lang])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.tgt[index]\n",
    "\n",
    "train_pc = ParallelCorpus(train_data, src_lang='en', tgt_lang='hi')\n",
    "test_pc = ParallelCorpus(test_data, src_lang='en', tgt_lang='hi')\n",
    "val_pc = ParallelCorpus(val_data, src_lang='en', tgt_lang='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334ce983",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_prefix = 100\n",
    "lr = 1e-4\n",
    "batch_size = 4\n",
    "num_epochs = 1\n",
    "token_limit = (1023 - len_prefix) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b1862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_pc, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_pc, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_pc, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811c43fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'T5Tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MT5Tokenizer.from_pretrained(\"THUMT/mGPT\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"THUMT/mGPT\")\n",
    "\n",
    "MT_model = LLM(model).to(device)\n",
    "optimizer = torch.optim.Adam(params=MT_model.parameters(),lr=lr, eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca21ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------EPOCH 1-------------------------------\n",
      "Step 500 | Val Loss: 4.29350| Best val loss: 4.29350 | Time:  0.0406 hrs\n",
      "Step 1000 | Val Loss: 3.50777| Best val loss: 3.50777 | Time:  0.0847 hrs\n",
      "Step 1500 | Val Loss: 3.48872| Best val loss: 3.48872 | Time:  0.1283 hrs\n",
      "Step 2000 | Val Loss: 3.70152| Best val loss: 3.48872 | Time:  0.1720 hrs\n",
      "Step 2500 | Val Loss: 3.20066| Best val loss: 3.20066 | Time:  0.2121 hrs\n",
      "Step 3000 | Val Loss: 3.70233| Best val loss: 3.20066 | Time:  0.2541 hrs\n",
      "Step 3500 | Val Loss: 4.37849| Best val loss: 3.20066 | Time:  0.2943 hrs\n",
      "Step 4000 | Val Loss: 3.22966| Best val loss: 3.20066 | Time:  0.3329 hrs\n",
      "Step 4500 | Val Loss: 3.56074| Best val loss: 3.20066 | Time:  0.3717 hrs\n",
      "Step 5000 | Val Loss: 3.14477| Best val loss: 3.14477 | Time:  0.4102 hrs\n",
      "Step 5500 | Val Loss: 4.06344| Best val loss: 3.14477 | Time:  0.4510 hrs\n",
      "Step 6000 | Val Loss: 3.07499| Best val loss: 3.07499 | Time:  0.4890 hrs\n",
      "Step 6500 | Val Loss: 4.30031| Best val loss: 3.07499 | Time:  0.5294 hrs\n",
      "Step 7000 | Val Loss: 3.10681| Best val loss: 3.07499 | Time:  0.5664 hrs\n",
      "Step 7500 | Val Loss: 3.16353| Best val loss: 3.07499 | Time:  0.6041 hrs\n",
      "Step 8000 | Val Loss: 3.84007| Best val loss: 3.07499 | Time:  0.6402 hrs\n",
      "Step 8500 | Val Loss: 2.94289| Best val loss: 2.94289 | Time:  0.6772 hrs\n",
      "Step 9000 | Val Loss: 3.48030| Best val loss: 2.94289 | Time:  0.7164 hrs\n",
      "Step 9500 | Val Loss: 3.47285| Best val loss: 2.94289 | Time:  0.7524 hrs\n",
      "Step 10000 | Val Loss: 2.98754| Best val loss: 2.94289 | Time:  0.7926 hrs\n",
      "Step 10500 | Val Loss: 4.29117| Best val loss: 2.94289 | Time:  0.8307 hrs\n",
      "Step 11000 | Val Loss: 3.03753| Best val loss: 2.94289 | Time:  0.8689 hrs\n",
      "Step 11500 | Val Loss: 3.02749| Best val loss: 2.94289 | Time:  0.9045 hrs\n",
      "Step 12000 | Val Loss: 3.76681| Best val loss: 2.94289 | Time:  0.9398 hrs\n",
      "Step 12500 | Val Loss: 2.86735| Best val loss: 2.86735 | Time:  0.9779 hrs\n",
      "Step 13000 | Val Loss: 2.87735| Best val loss: 2.86735 | Time:  1.0158 hrs\n",
      "Step 13500 | Val Loss: 3.58557| Best val loss: 2.86735 | Time:  1.0509 hrs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation():\n",
    "    total_loss = 0\n",
    "    for i, (src, tgt) in enumerate(val_loader):\n",
    "        max_src_len = min(token_limit, max([len(s) for s in src]))\n",
    "        max_tgt_len = min(token_limit, max([len(s) for s in tgt]))\n",
    "        inputs = tokenizer(src, padding='max_length', truncation=True, max_length=max_src_len)\n",
    "        targets = tokenizer(tgt, padding='max_length', truncation=True, max_length=max_tgt_len)\n",
    "        input_ids, input_masks = inputs['input_ids'], inputs['attention_mask']\n",
    "        target_ids, target_masks = targets['input_ids'], targets['attention_mask']\n",
    "        for j in range(len(target_ids)):\n",
    "            target_ids[j].insert(0, 1)\n",
    "            target_masks[j].insert(0, 1)\n",
    "        input_ids, input_masks = torch.tensor(input_ids).to(device), torch.tensor(input_masks).to(device)\n",
    "        target_ids, target_masks = torch.tensor(target_ids).to(device), torch.tensor(target_masks).to(device)\n",
    "        loss = MT_model(input_ids, input_masks, target_ids, target_masks)\n",
    "        total_loss += loss\n",
    "    return total_loss / len(val_loader)\n",
    "        \n",
    "\n",
    "min_val_loss = 10000\n",
    "PATH = 'saved_models/finetune.pt'\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"------------------------EPOCH {epoch + 1}-------------------------------\")\n",
    "    t1 = time.time()\n",
    "    for i, (src, tgt) in enumerate(train_loader):\n",
    "        MT_model.zero_grad()\n",
    "        \n",
    "        max_src_len = min(token_limit, max([len(s) for s in src]))\n",
    "        max_tgt_len = min(token_limit, max([len(s) for s in tgt]))\n",
    "        inputs = tokenizer(src, padding='max_length', truncation=True, max_length=max_src_len)\n",
    "        targets = tokenizer(tgt, padding='max_length', truncation=True, max_length=max_tgt_len)\n",
    "        input_ids, input_masks = inputs['input_ids'], inputs['attention_mask']\n",
    "        target_ids, target_masks = targets['input_ids'], targets['attention_mask']\n",
    "#         print(len(input_ids[0]))\n",
    "        for j in range(len(target_ids)):\n",
    "            target_ids[j].insert(0, 1)\n",
    "            target_masks[j].insert(0, 1)\n",
    "#         print(len(input_ids[0]))\n",
    "#         print(MT_model._model.config.max_position_embeddings)\n",
    "#         print(tgt[0], target_ids[0])\n",
    "        \n",
    "        input_ids, input_masks = torch.tensor(input_ids).to(device), torch.tensor(input_masks).to(device)\n",
    "        target_ids, target_masks = torch.tensor(target_ids).to(device), torch.tensor(target_masks).to(device)\n",
    "        loss = MT_model(input_ids, input_masks, target_ids, target_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1)%500 == 0:\n",
    "            t2 = time.time()\n",
    "            val_loss = validation()\n",
    "            if val_loss.item() < min_val_loss:\n",
    "                torch.save(MT_model.state_dict(), PATH)\n",
    "                min_val_loss = val_loss\n",
    "            print(f'Step {i+1} | Val Loss: {val_loss.item():.5f}| Best val loss: {min_val_loss:.5f} | Time: {(t2-t1)/3600 : .4f} hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a28c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Multi-stage-prompting
In this work, I have re-implemented the paper [MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators](https://aclanthology.org/2022.acl-long.424.pdf). 
The aim of this work is to showcasse parameter efficient training for Neural Machine Transaltion using decoder-based pre-trained language models. We are using the [CFILT English-Hindi parallel corpus](https://www.cfilt.iitb.ac.in/iitb_parallel/) for training our model adn evaluate it on WMT14 dev and test sets.
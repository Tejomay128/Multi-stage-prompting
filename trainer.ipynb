{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from multi_stage import LLM\n",
    "from prep_data import get_eng_hi_dataset\n",
    "from transformers import GPT2LMHeadModel, MT5Tokenizer\n",
    "\n",
    "device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# torch.backends.cudnn.enabled = True\n",
    "# torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Obtain parallel data (EN-HI)</h2>\n",
    "<h5>Data is in the form of dictionary with 'en' and 'hi' keys corresponding to english and hindi sentences respectively</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/cfilt___parquet/cfilt--iitb-english-hindi-911387c6837f8b91/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439382c88e9d4e09afcc4c62ecdb260d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data, val_data, test_data = get_eng_hi_dataset()\n",
    "train_data = train_data[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1611623, 520, 2507)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelCorpus(Dataset):\n",
    "    def __init__(self, data, src_lang='en', tgt_lang='hi') -> None:\n",
    "        super(ParallelCorpus, self).__init__()\n",
    "        self.src = []\n",
    "        self.tgt = []\n",
    "        for pair in data:\n",
    "            self.src.append(pair[src_lang])\n",
    "            self.tgt.append(pair[tgt_lang])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.tgt[index]\n",
    "\n",
    "train_pc = ParallelCorpus(train_data, src_lang='en', tgt_lang='hi')\n",
    "test_pc = ParallelCorpus(test_data, src_lang='en', tgt_lang='hi')\n",
    "val_pc = ParallelCorpus(val_data, src_lang='en', tgt_lang='hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hyperparameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_prefix = 100\n",
    "lr = 1e-4\n",
    "beta1 = 0.9\n",
    "beta2 = 0.98\n",
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "token_limit = (1023 - len_prefix) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201453"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset=train_pc, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_pc, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_pc, batch_size=1, shuffle=False)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'T5Tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MT5Tokenizer.from_pretrained(\"THUMT/mGPT\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"THUMT/mGPT\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "MT_model = LLM(model, len_prefix).to(device)\n",
    "optimizer = torch.optim.Adam(params=MT_model.prefix.parameters(),lr=lr, betas=(beta1, beta2), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------EPOCH 1-------------------------------\n",
      "Step 250 | Val Loss: 4.12811| Best val loss: 4.12811 | Time:  0.1647 hrs\n",
      "Step 500 | Val Loss: 3.81849| Best val loss: 3.81849 | Time:  0.3378 hrs\n",
      "Step 750 | Val Loss: 3.68523| Best val loss: 3.68523 | Time:  0.5067 hrs\n",
      "Step 1000 | Val Loss: 3.64871| Best val loss: 3.64871 | Time:  0.6795 hrs\n",
      "Step 1250 | Val Loss: 3.64961| Best val loss: 3.64871 | Time:  0.8471 hrs\n",
      "Step 1500 | Val Loss: 3.67446| Best val loss: 3.64871 | Time:  1.0073 hrs\n",
      "Step 1750 | Val Loss: 3.78017| Best val loss: 3.64871 | Time:  1.1725 hrs\n",
      "Step 2000 | Val Loss: 3.89772| Best val loss: 3.64871 | Time:  1.3281 hrs\n",
      "Step 2250 | Val Loss: 3.91310| Best val loss: 3.64871 | Time:  1.4855 hrs\n",
      "Step 2500 | Val Loss: 4.01997| Best val loss: 3.64871 | Time:  1.6414 hrs\n",
      "Step 2750 | Val Loss: 4.00877| Best val loss: 3.64871 | Time:  1.7955 hrs\n",
      "Step 3000 | Val Loss: 4.04939| Best val loss: 3.64871 | Time:  1.9484 hrs\n",
      "Step 3250 | Val Loss: 4.13394| Best val loss: 3.64871 | Time:  2.1003 hrs\n",
      "Step 3500 | Val Loss: 4.11609| Best val loss: 3.64871 | Time:  2.2487 hrs\n",
      "Step 3750 | Val Loss: 4.24485| Best val loss: 3.64871 | Time:  2.4004 hrs\n",
      "Step 4000 | Val Loss: 4.15233| Best val loss: 3.64871 | Time:  2.5448 hrs\n",
      "Step 4250 | Val Loss: 4.19090| Best val loss: 3.64871 | Time:  2.6942 hrs\n",
      "Step 4500 | Val Loss: 4.18397| Best val loss: 3.64871 | Time:  2.8408 hrs\n",
      "Step 4750 | Val Loss: 4.13545| Best val loss: 3.64871 | Time:  2.9847 hrs\n",
      "Step 5000 | Val Loss: 4.22462| Best val loss: 3.64871 | Time:  3.1299 hrs\n",
      "Step 5250 | Val Loss: 4.15246| Best val loss: 3.64871 | Time:  3.2731 hrs\n",
      "Step 5500 | Val Loss: 4.19815| Best val loss: 3.64871 | Time:  3.4167 hrs\n",
      "Step 5750 | Val Loss: 4.17680| Best val loss: 3.64871 | Time:  3.5591 hrs\n",
      "Step 6000 | Val Loss: 4.13640| Best val loss: 3.64871 | Time:  3.6992 hrs\n",
      "Step 6250 | Val Loss: 4.07352| Best val loss: 3.64871 | Time:  3.8374 hrs\n",
      "Step 6500 | Val Loss: 4.09895| Best val loss: 3.64871 | Time:  3.9766 hrs\n",
      "Step 6750 | Val Loss: 4.12510| Best val loss: 3.64871 | Time:  4.1151 hrs\n",
      "Step 7000 | Val Loss: 4.05728| Best val loss: 3.64871 | Time:  4.2500 hrs\n",
      "Step 7250 | Val Loss: 4.07765| Best val loss: 3.64871 | Time:  4.3881 hrs\n",
      "Step 7500 | Val Loss: 4.11321| Best val loss: 3.64871 | Time:  4.5270 hrs\n",
      "Step 7750 | Val Loss: 4.09826| Best val loss: 3.64871 | Time:  4.6597 hrs\n",
      "Step 8000 | Val Loss: 4.06418| Best val loss: 3.64871 | Time:  4.7945 hrs\n",
      "Step 8250 | Val Loss: 4.03756| Best val loss: 3.64871 | Time:  4.9295 hrs\n",
      "Step 8500 | Val Loss: 4.06336| Best val loss: 3.64871 | Time:  5.0666 hrs\n",
      "Step 8750 | Val Loss: 3.99807| Best val loss: 3.64871 | Time:  5.1955 hrs\n",
      "Step 9000 | Val Loss: 3.93645| Best val loss: 3.64871 | Time:  5.3281 hrs\n",
      "Step 9250 | Val Loss: 3.89483| Best val loss: 3.64871 | Time:  5.4631 hrs\n",
      "Step 9500 | Val Loss: 3.93346| Best val loss: 3.64871 | Time:  5.5963 hrs\n",
      "Step 9750 | Val Loss: 3.95148| Best val loss: 3.64871 | Time:  5.7271 hrs\n",
      "Step 10000 | Val Loss: 3.83104| Best val loss: 3.64871 | Time:  5.8573 hrs\n",
      "Step 10250 | Val Loss: 3.89425| Best val loss: 3.64871 | Time:  5.9925 hrs\n",
      "Step 10500 | Val Loss: 3.85767| Best val loss: 3.64871 | Time:  6.1229 hrs\n",
      "Step 10750 | Val Loss: 3.93915| Best val loss: 3.64871 | Time:  6.2581 hrs\n",
      "Step 11000 | Val Loss: 3.92256| Best val loss: 3.64871 | Time:  6.3850 hrs\n",
      "Step 11250 | Val Loss: 3.80248| Best val loss: 3.64871 | Time:  6.5149 hrs\n",
      "Step 11500 | Val Loss: 3.80996| Best val loss: 3.64871 | Time:  6.6480 hrs\n",
      "Step 11750 | Val Loss: 3.77626| Best val loss: 3.64871 | Time:  6.7776 hrs\n",
      "Step 12000 | Val Loss: 3.90309| Best val loss: 3.64871 | Time:  6.9107 hrs\n",
      "Step 12250 | Val Loss: 3.81009| Best val loss: 3.64871 | Time:  7.0368 hrs\n",
      "Step 12500 | Val Loss: 3.68728| Best val loss: 3.64871 | Time:  7.1641 hrs\n",
      "Step 12750 | Val Loss: 3.73314| Best val loss: 3.64871 | Time:  7.2959 hrs\n",
      "Step 13000 | Val Loss: 3.63637| Best val loss: 3.63637 | Time:  7.4230 hrs\n",
      "Step 13250 | Val Loss: 3.67280| Best val loss: 3.63637 | Time:  7.5549 hrs\n",
      "Step 13500 | Val Loss: 3.76962| Best val loss: 3.63637 | Time:  7.6844 hrs\n",
      "Step 13750 | Val Loss: 3.61943| Best val loss: 3.61943 | Time:  7.8092 hrs\n",
      "Step 14000 | Val Loss: 3.56188| Best val loss: 3.56188 | Time:  7.9398 hrs\n",
      "Step 14250 | Val Loss: 3.54351| Best val loss: 3.54351 | Time:  8.0715 hrs\n",
      "Step 14500 | Val Loss: 3.55604| Best val loss: 3.54351 | Time:  8.1980 hrs\n",
      "Step 14750 | Val Loss: 3.54664| Best val loss: 3.54351 | Time:  8.3266 hrs\n",
      "Step 15000 | Val Loss: 3.60306| Best val loss: 3.54351 | Time:  8.4540 hrs\n",
      "Step 15250 | Val Loss: 3.56932| Best val loss: 3.54351 | Time:  8.5771 hrs\n",
      "Step 15500 | Val Loss: 3.46694| Best val loss: 3.46694 | Time:  8.7026 hrs\n",
      "Step 15750 | Val Loss: 3.44954| Best val loss: 3.44954 | Time:  8.8338 hrs\n",
      "Step 16000 | Val Loss: 3.38820| Best val loss: 3.38820 | Time:  8.9631 hrs\n",
      "Step 16250 | Val Loss: 3.35552| Best val loss: 3.35552 | Time:  9.0914 hrs\n",
      "Step 16500 | Val Loss: 3.51231| Best val loss: 3.35552 | Time:  9.2232 hrs\n",
      "Step 16750 | Val Loss: 3.44816| Best val loss: 3.35552 | Time:  9.3460 hrs\n",
      "Step 17000 | Val Loss: 3.38262| Best val loss: 3.35552 | Time:  9.4676 hrs\n",
      "Step 17250 | Val Loss: 3.33766| Best val loss: 3.33766 | Time:  9.5929 hrs\n",
      "Step 17500 | Val Loss: 3.23507| Best val loss: 3.23507 | Time:  9.7232 hrs\n",
      "Step 17750 | Val Loss: 3.22443| Best val loss: 3.22443 | Time:  9.8493 hrs\n",
      "Step 18000 | Val Loss: 3.26439| Best val loss: 3.22443 | Time:  9.9761 hrs\n",
      "Step 18250 | Val Loss: 3.32341| Best val loss: 3.22443 | Time:  10.1029 hrs\n",
      "Step 18500 | Val Loss: 3.21764| Best val loss: 3.21764 | Time:  10.2246 hrs\n",
      "Step 18750 | Val Loss: 3.16956| Best val loss: 3.16956 | Time:  10.3495 hrs\n",
      "Step 19000 | Val Loss: 3.18181| Best val loss: 3.16956 | Time:  10.4749 hrs\n",
      "Step 19250 | Val Loss: 3.13440| Best val loss: 3.13440 | Time:  10.5992 hrs\n",
      "Step 19500 | Val Loss: 3.11375| Best val loss: 3.11375 | Time:  10.7251 hrs\n",
      "Step 19750 | Val Loss: 3.13996| Best val loss: 3.11375 | Time:  10.8491 hrs\n",
      "Step 20000 | Val Loss: 3.09897| Best val loss: 3.09897 | Time:  10.9728 hrs\n",
      "Step 20250 | Val Loss: 3.13716| Best val loss: 3.09897 | Time:  11.1014 hrs\n",
      "Step 20500 | Val Loss: 3.08793| Best val loss: 3.08793 | Time:  11.2200 hrs\n",
      "Step 20750 | Val Loss: 3.01167| Best val loss: 3.01167 | Time:  11.3421 hrs\n",
      "Step 21000 | Val Loss: 3.04145| Best val loss: 3.01167 | Time:  11.4670 hrs\n",
      "Step 21250 | Val Loss: 2.98164| Best val loss: 2.98164 | Time:  11.5899 hrs\n",
      "Step 21500 | Val Loss: 2.91326| Best val loss: 2.91326 | Time:  11.7143 hrs\n",
      "Step 21750 | Val Loss: 2.96009| Best val loss: 2.91326 | Time:  11.8286 hrs\n",
      "Step 22000 | Val Loss: 2.95552| Best val loss: 2.91326 | Time:  11.9400 hrs\n",
      "Step 22250 | Val Loss: 3.15126| Best val loss: 2.91326 | Time:  12.0598 hrs\n",
      "Step 22500 | Val Loss: 2.96379| Best val loss: 2.91326 | Time:  12.1735 hrs\n",
      "Step 22750 | Val Loss: 3.01565| Best val loss: 2.91326 | Time:  12.2869 hrs\n",
      "Step 23000 | Val Loss: 2.93394| Best val loss: 2.91326 | Time:  12.4015 hrs\n",
      "Step 23250 | Val Loss: 2.86882| Best val loss: 2.86882 | Time:  12.5182 hrs\n",
      "Step 23500 | Val Loss: 2.90525| Best val loss: 2.86882 | Time:  12.6386 hrs\n",
      "Step 23750 | Val Loss: 2.83840| Best val loss: 2.83840 | Time:  12.7523 hrs\n",
      "Step 24000 | Val Loss: 2.82610| Best val loss: 2.82610 | Time:  12.8685 hrs\n",
      "Step 24250 | Val Loss: 2.83893| Best val loss: 2.82610 | Time:  12.9875 hrs\n",
      "Step 24500 | Val Loss: 3.02690| Best val loss: 2.82610 | Time:  13.1050 hrs\n",
      "Step 24750 | Val Loss: 2.84697| Best val loss: 2.82610 | Time:  13.2182 hrs\n",
      "Step 25000 | Val Loss: 2.84835| Best val loss: 2.82610 | Time:  13.3298 hrs\n",
      "Step 25250 | Val Loss: 2.73265| Best val loss: 2.73265 | Time:  13.4424 hrs\n",
      "Step 25500 | Val Loss: 2.75996| Best val loss: 2.73265 | Time:  13.5604 hrs\n",
      "Step 25750 | Val Loss: 2.70364| Best val loss: 2.70364 | Time:  13.6761 hrs\n",
      "Step 26000 | Val Loss: 2.64005| Best val loss: 2.64005 | Time:  13.7922 hrs\n",
      "Step 26250 | Val Loss: 2.66065| Best val loss: 2.64005 | Time:  13.9066 hrs\n",
      "Step 26500 | Val Loss: 2.73443| Best val loss: 2.64005 | Time:  14.0197 hrs\n",
      "Step 26750 | Val Loss: 2.74512| Best val loss: 2.64005 | Time:  14.1345 hrs\n",
      "Step 27000 | Val Loss: 2.74459| Best val loss: 2.64005 | Time:  14.2507 hrs\n",
      "Step 27250 | Val Loss: 2.78778| Best val loss: 2.64005 | Time:  14.3610 hrs\n",
      "Step 27500 | Val Loss: 2.77546| Best val loss: 2.64005 | Time:  14.4709 hrs\n",
      "Step 27750 | Val Loss: 2.71679| Best val loss: 2.64005 | Time:  14.5834 hrs\n",
      "Step 28000 | Val Loss: 2.65520| Best val loss: 2.64005 | Time:  14.6953 hrs\n",
      "Step 28250 | Val Loss: 2.64708| Best val loss: 2.64005 | Time:  14.8082 hrs\n",
      "Step 28500 | Val Loss: 2.60864| Best val loss: 2.60864 | Time:  14.9202 hrs\n",
      "Step 28750 | Val Loss: 2.61258| Best val loss: 2.60864 | Time:  15.0337 hrs\n",
      "Step 29000 | Val Loss: 2.60250| Best val loss: 2.60250 | Time:  15.1446 hrs\n",
      "Step 29250 | Val Loss: 2.63047| Best val loss: 2.60250 | Time:  15.2609 hrs\n",
      "Step 29500 | Val Loss: 2.68198| Best val loss: 2.60250 | Time:  15.3767 hrs\n",
      "Step 29750 | Val Loss: 2.67831| Best val loss: 2.60250 | Time:  15.4893 hrs\n",
      "Step 30000 | Val Loss: 2.67950| Best val loss: 2.60250 | Time:  15.5991 hrs\n",
      "Step 30250 | Val Loss: 2.60396| Best val loss: 2.60250 | Time:  15.7095 hrs\n",
      "Step 30500 | Val Loss: 2.60347| Best val loss: 2.60250 | Time:  15.8228 hrs\n",
      "Step 30750 | Val Loss: 2.56574| Best val loss: 2.56574 | Time:  15.9354 hrs\n",
      "Step 31000 | Val Loss: 2.60613| Best val loss: 2.56574 | Time:  16.0529 hrs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation():\n",
    "    total_loss = 0\n",
    "    for i, (src, tgt) in enumerate(val_loader):\n",
    "        max_src_len = min(token_limit, max([len(s) for s in src]))\n",
    "        max_tgt_len = min(token_limit, max([len(s) for s in tgt]))\n",
    "        inputs = tokenizer(src, padding='max_length', truncation=True, max_length=max_src_len)\n",
    "        targets = tokenizer(tgt, padding='max_length', truncation=True, max_length=max_tgt_len)\n",
    "        input_ids, input_masks = inputs['input_ids'], inputs['attention_mask']\n",
    "        target_ids, target_masks = targets['input_ids'], targets['attention_mask']\n",
    "        for j in range(len(target_ids)):\n",
    "            target_ids[j].insert(0, 1)\n",
    "            target_masks[j].insert(0, 1)\n",
    "        input_ids, input_masks = torch.tensor(input_ids).to(device), torch.tensor(input_masks).to(device)\n",
    "        target_ids, target_masks = torch.tensor(target_ids).to(device), torch.tensor(target_masks).to(device)\n",
    "        loss = MT_model(input_ids, input_masks, target_ids, target_masks)\n",
    "        total_loss += loss\n",
    "    return total_loss / len(val_loader)\n",
    "        \n",
    "\n",
    "min_val_loss = 10000\n",
    "PATH = 'saved_models/prefix.pt'\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"------------------------EPOCH {epoch + 1}-------------------------------\")\n",
    "    t1 = time.time()\n",
    "    for i, (src, tgt) in enumerate(train_loader):\n",
    "        MT_model.zero_grad()\n",
    "        \n",
    "        max_src_len = min(token_limit, max([len(s) for s in src]))\n",
    "        max_tgt_len = min(token_limit, max([len(s) for s in tgt]))\n",
    "        inputs = tokenizer(src, padding='max_length', truncation=True, max_length=max_src_len)\n",
    "        targets = tokenizer(tgt, padding='max_length', truncation=True, max_length=max_tgt_len)\n",
    "        input_ids, input_masks = inputs['input_ids'], inputs['attention_mask']\n",
    "        target_ids, target_masks = targets['input_ids'], targets['attention_mask']\n",
    "#         print(len(input_ids[0]))\n",
    "        for j in range(len(target_ids)):\n",
    "            target_ids[j].insert(0, 1)\n",
    "            target_masks[j].insert(0, 1)\n",
    "#         print(len(input_ids[0]))\n",
    "#         print(MT_model._model.config.max_position_embeddings)\n",
    "#         print(tgt[0], target_ids[0])\n",
    "        \n",
    "        input_ids, input_masks = torch.tensor(input_ids).to(device), torch.tensor(input_masks).to(device)\n",
    "        target_ids, target_masks = torch.tensor(target_ids).to(device), torch.tensor(target_masks).to(device)\n",
    "        loss = MT_model(input_ids, input_masks, target_ids, target_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1)%250 == 0:\n",
    "            t2 = time.time()\n",
    "            val_loss = validation()\n",
    "            if val_loss.item() < min_val_loss:\n",
    "                torch.save(MT_model.prefix.state_dict(), PATH)\n",
    "                min_val_loss = val_loss\n",
    "            print(f'Step {i+1} | Val Loss: {val_loss.item():.5f}| Best val loss: {min_val_loss:.5f} | Time: {(t2-t1)/3600 : .4f} hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"saved_models/prefix.pt\"\n",
    "MT_model.prefix.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation():\n",
    "    total_loss = 0\n",
    "    for i, (src, tgt) in enumerate(val_loader):\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f\"Processing {i+1}....\")\n",
    "        max_src_len = min(token_limit, max([len(s) for s in src]))\n",
    "        max_tgt_len = min(token_limit, max([len(s) for s in tgt]))\n",
    "        inputs = tokenizer(src, padding='max_length', truncation=True, max_length=max_src_len)\n",
    "        targets = tokenizer(tgt, padding='max_length', truncation=True, max_length=max_tgt_len)\n",
    "        input_ids, input_masks = inputs['input_ids'], inputs['attention_mask']\n",
    "        target_ids, target_masks = targets['input_ids'], targets['attention_mask']\n",
    "        for j in range(len(target_ids)):\n",
    "            target_ids[j].insert(0, 1)\n",
    "            target_masks[j].insert(0, 1)\n",
    "        input_ids, input_masks = torch.tensor(input_ids).to(device), torch.tensor(input_masks).to(device)\n",
    "        target_ids, target_masks = torch.tensor(target_ids).to(device), torch.tensor(target_masks).to(device)\n",
    "        loss = MT_model(input_ids, input_masks, target_ids, target_masks)\n",
    "        total_loss += loss\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_translate(model, device, tokenizer, input_sent):\n",
    "    tok_output = tokenizer(input_sent)\n",
    "    input_ids = tok_output['input_ids']\n",
    "    input_mask = tok_output['attention_mask']\n",
    "    input_ids = torch.tensor(input_ids, device=device)\n",
    "    input_mask = torch.tensor(input_mask, device=device)\n",
    "    target_mask = torch.ones([1, 1], device=device)\n",
    "    past_key_values = model.encode(input_ids, input_mask)\n",
    "    print(past_key_values[0][0].shape)\n",
    "    start = [1]\n",
    "    gen = []\n",
    "    curr_token = None\n",
    "    while curr_token != 1:\n",
    "        tgt = torch.tensor(start, device=device)    \n",
    "        logits, past_key_values = model.decode(tgt, input_mask, target_mask, past_key_values)\n",
    "        print(past_key_values[0][0].shape)\n",
    "        logits = model.logSoftmax(logits.unsqueeze(0)).squeeze(0)\n",
    "        value, index = torch.max(logits, dim=1)\n",
    "        curr_token = index[0].item()\n",
    "        gen.append(curr_token)\n",
    "        start = [curr_token]\n",
    "        print(curr_token, value.item())\n",
    "        target_mask = torch.cat([target_mask, torch.ones([1, 1], device=device)], dim=1)\n",
    "    output_sent = tokenizer.decode(gen)\n",
    "    return output_sent\n",
    "\n",
    "sent = ['is it okay to have a bank account']\n",
    "greedy_translate(MT_model, device, tokenizer, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('</s>')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba129d3ed69af87e4eab8c2d4e17f59a651f25d15035729502e57aff85cb642c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

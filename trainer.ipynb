{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from multi_stage import LLM\n",
    "from prep_data import get_eng_hi_dataset\n",
    "from transformers import GPT2LMHeadModel, MT5Tokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# torch.backends.cudnn.enabled = True\n",
    "# torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Obtain parallel data (EN-HI)</h2>\n",
    "<h5>Data is in the form of dictionary with 'en' and 'hi' keys corresponding to english and hindi sentences respectively</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/cfilt___parquet/cfilt--iitb-english-hindi-911387c6837f8b91/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e703d1159b9644f18da646f2f2a83241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_data(PATH):\n",
    "    dataset = []\n",
    "    f_en = open(PATH + 'filtered.en', 'r')\n",
    "    for line in f_en.readlines():\n",
    "        line = line.strip('\\n')\n",
    "        entry = {'en': line}\n",
    "        dataset.append(entry)\n",
    "    f_en.close()\n",
    "    \n",
    "    f_hi = open(PATH + 'filtered.hi', 'r')\n",
    "    for i, line in enumerate(f_hi.readlines()):\n",
    "        line = line.strip('\\n')\n",
    "        dataset[i]['hi'] = line\n",
    "    f_hi.close()\n",
    "    return dataset\n",
    "\n",
    "val_data, test_data = get_eng_hi_dataset()\n",
    "train_data = read_data('filtered_data/')\n",
    "train_data = train_data[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 520, 2507)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelCorpus(Dataset):\n",
    "    def __init__(self, data, src_lang='en', tgt_lang='hi') -> None:\n",
    "        super(ParallelCorpus, self).__init__()\n",
    "        self.src = []\n",
    "        self.tgt = []\n",
    "        for pair in data:\n",
    "            self.src.append(pair[src_lang])\n",
    "            self.tgt.append(pair[tgt_lang])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.tgt[index]\n",
    "\n",
    "train_pc = ParallelCorpus(train_data, src_lang='en', tgt_lang='hi')\n",
    "test_pc = ParallelCorpus(test_data, src_lang='en', tgt_lang='hi')\n",
    "val_pc = ParallelCorpus(val_data, src_lang='en', tgt_lang='hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hyperparameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_prefix = 100\n",
    "lr = 1e-4\n",
    "beta1 = 0.9\n",
    "beta2 = 0.98\n",
    "batch_size = 4\n",
    "num_epochs = 2\n",
    "token_limit = ((1023 - len_prefix) // 2) - 3  #to accomodate extra one token if max_len=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset=train_pc, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_pc, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_pc, batch_size=1, shuffle=False)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'T5Tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MT5Tokenizer.from_pretrained(\"THUMT/mGPT\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"THUMT/mGPT\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "MT_model = LLM(model, len_prefix).to(device)\n",
    "optimizer = torch.optim.Adam(params=MT_model.prefix.parameters(),lr=lr, betas=(beta1, beta2), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------EPOCH 1-------------------------------\n",
      "Step 500 | Val Loss: 3.56302| Best val loss: 3.56302 | Time:  0.0666 hrs\n",
      "Step 1000 | Val Loss: 3.26050| Best val loss: 3.26050 | Time:  0.1415 hrs\n",
      "Step 1500 | Val Loss: 3.03638| Best val loss: 3.03638 | Time:  0.2180 hrs\n",
      "Step 2000 | Val Loss: 2.81596| Best val loss: 2.81596 | Time:  0.2950 hrs\n",
      "Step 2500 | Val Loss: 2.61167| Best val loss: 2.61167 | Time:  0.3718 hrs\n",
      "Step 3000 | Val Loss: 2.55078| Best val loss: 2.55078 | Time:  0.4489 hrs\n",
      "Step 3500 | Val Loss: 2.51563| Best val loss: 2.51563 | Time:  0.5265 hrs\n",
      "Step 4000 | Val Loss: 2.48602| Best val loss: 2.48602 | Time:  0.6040 hrs\n",
      "Step 4500 | Val Loss: 2.46205| Best val loss: 2.46205 | Time:  0.6821 hrs\n",
      "Step 5000 | Val Loss: 2.43383| Best val loss: 2.43383 | Time:  0.7618 hrs\n",
      "Step 5500 | Val Loss: 2.41379| Best val loss: 2.41379 | Time:  0.8426 hrs\n",
      "Step 6000 | Val Loss: 2.40198| Best val loss: 2.40198 | Time:  0.9236 hrs\n",
      "Step 6500 | Val Loss: 2.39013| Best val loss: 2.39013 | Time:  1.0029 hrs\n",
      "Step 7000 | Val Loss: 2.37572| Best val loss: 2.37572 | Time:  1.0835 hrs\n",
      "Step 7500 | Val Loss: 2.35758| Best val loss: 2.35758 | Time:  1.1640 hrs\n",
      "Step 8000 | Val Loss: 2.34932| Best val loss: 2.34932 | Time:  1.2442 hrs\n",
      "Step 8500 | Val Loss: 2.34631| Best val loss: 2.34631 | Time:  1.3258 hrs\n",
      "Step 9000 | Val Loss: 2.33043| Best val loss: 2.33043 | Time:  1.4081 hrs\n",
      "Step 9500 | Val Loss: 2.33174| Best val loss: 2.33043 | Time:  1.4878 hrs\n",
      "Step 10000 | Val Loss: 2.31552| Best val loss: 2.31552 | Time:  1.5670 hrs\n",
      "Step 10500 | Val Loss: 2.30724| Best val loss: 2.30724 | Time:  1.6501 hrs\n",
      "Step 11000 | Val Loss: 2.30418| Best val loss: 2.30418 | Time:  1.7342 hrs\n",
      "Step 11500 | Val Loss: 2.29878| Best val loss: 2.29878 | Time:  1.8191 hrs\n",
      "Step 12000 | Val Loss: 2.29183| Best val loss: 2.29183 | Time:  1.9040 hrs\n",
      "Step 12500 | Val Loss: 2.28479| Best val loss: 2.28479 | Time:  1.9878 hrs\n",
      "Step 13000 | Val Loss: 2.27836| Best val loss: 2.27836 | Time:  2.0725 hrs\n",
      "Step 13500 | Val Loss: 2.26768| Best val loss: 2.26768 | Time:  2.1578 hrs\n",
      "Step 14000 | Val Loss: 2.26654| Best val loss: 2.26654 | Time:  2.2395 hrs\n",
      "Step 14500 | Val Loss: 2.25782| Best val loss: 2.25782 | Time:  2.3231 hrs\n",
      "Step 15000 | Val Loss: 2.25393| Best val loss: 2.25393 | Time:  2.4047 hrs\n",
      "Step 15500 | Val Loss: 2.25347| Best val loss: 2.25347 | Time:  2.4888 hrs\n",
      "Step 16000 | Val Loss: 2.25918| Best val loss: 2.25347 | Time:  2.5731 hrs\n",
      "Step 16500 | Val Loss: 2.24223| Best val loss: 2.24223 | Time:  2.6526 hrs\n",
      "Step 17000 | Val Loss: 2.24079| Best val loss: 2.24079 | Time:  2.7343 hrs\n",
      "Step 17500 | Val Loss: 2.23888| Best val loss: 2.23888 | Time:  2.8206 hrs\n",
      "Step 18000 | Val Loss: 2.23243| Best val loss: 2.23243 | Time:  2.9054 hrs\n",
      "Step 18500 | Val Loss: 2.22445| Best val loss: 2.22445 | Time:  2.9912 hrs\n",
      "Step 19000 | Val Loss: 2.22146| Best val loss: 2.22146 | Time:  3.0775 hrs\n",
      "Step 19500 | Val Loss: 2.22091| Best val loss: 2.22091 | Time:  3.1738 hrs\n",
      "Step 20000 | Val Loss: 2.21269| Best val loss: 2.21269 | Time:  3.2951 hrs\n",
      "Step 20500 | Val Loss: 2.20667| Best val loss: 2.20667 | Time:  3.4029 hrs\n",
      "Step 21000 | Val Loss: 2.21059| Best val loss: 2.20667 | Time:  3.5128 hrs\n",
      "Step 21500 | Val Loss: 2.20723| Best val loss: 2.20667 | Time:  3.6032 hrs\n",
      "Step 22000 | Val Loss: 2.20238| Best val loss: 2.20238 | Time:  3.7053 hrs\n",
      "Step 22500 | Val Loss: 2.20133| Best val loss: 2.20133 | Time:  3.8309 hrs\n",
      "Step 23000 | Val Loss: 2.19666| Best val loss: 2.19666 | Time:  3.9437 hrs\n",
      "Step 23500 | Val Loss: 2.19049| Best val loss: 2.19049 | Time:  4.0320 hrs\n",
      "Step 24000 | Val Loss: 2.19090| Best val loss: 2.19049 | Time:  4.1194 hrs\n",
      "Step 24500 | Val Loss: 2.19648| Best val loss: 2.19049 | Time:  4.2055 hrs\n",
      "Step 25000 | Val Loss: 2.18401| Best val loss: 2.18401 | Time:  4.2906 hrs\n",
      "------------------------EPOCH 2-------------------------------\n",
      "Step 500 | Val Loss: 2.18780| Best val loss: 2.18401 | Time:  0.0683 hrs\n",
      "Step 1000 | Val Loss: 2.19100| Best val loss: 2.18401 | Time:  0.1432 hrs\n",
      "Step 1500 | Val Loss: 2.19287| Best val loss: 2.18401 | Time:  0.2196 hrs\n",
      "Step 2000 | Val Loss: 2.19705| Best val loss: 2.18401 | Time:  0.2977 hrs\n",
      "Step 2500 | Val Loss: 2.19144| Best val loss: 2.18401 | Time:  0.3749 hrs\n",
      "Step 3000 | Val Loss: 2.19396| Best val loss: 2.18401 | Time:  0.4525 hrs\n",
      "Step 3500 | Val Loss: 2.19321| Best val loss: 2.18401 | Time:  0.5310 hrs\n",
      "Step 4000 | Val Loss: 2.19770| Best val loss: 2.18401 | Time:  0.6089 hrs\n",
      "Step 4500 | Val Loss: 2.19112| Best val loss: 2.18401 | Time:  0.6880 hrs\n",
      "Step 5000 | Val Loss: 2.18979| Best val loss: 2.18401 | Time:  0.7676 hrs\n",
      "Step 5500 | Val Loss: 2.18772| Best val loss: 2.18401 | Time:  0.8565 hrs\n",
      "Step 6000 | Val Loss: 2.19149| Best val loss: 2.18401 | Time:  0.9509 hrs\n",
      "Step 6500 | Val Loss: 2.19275| Best val loss: 2.18401 | Time:  1.0489 hrs\n",
      "Step 7000 | Val Loss: 2.18529| Best val loss: 2.18401 | Time:  1.1281 hrs\n",
      "Step 7500 | Val Loss: 2.18545| Best val loss: 2.18401 | Time:  1.2068 hrs\n",
      "Step 8000 | Val Loss: 2.18013| Best val loss: 2.18013 | Time:  1.2851 hrs\n",
      "Step 8500 | Val Loss: 2.18760| Best val loss: 2.18013 | Time:  1.3666 hrs\n",
      "Step 9000 | Val Loss: 2.17564| Best val loss: 2.17564 | Time:  1.4484 hrs\n",
      "Step 9500 | Val Loss: 2.18217| Best val loss: 2.17564 | Time:  1.5292 hrs\n",
      "Step 10000 | Val Loss: 2.17729| Best val loss: 2.17564 | Time:  1.6103 hrs\n",
      "Step 10500 | Val Loss: 2.17484| Best val loss: 2.17484 | Time:  1.6909 hrs\n",
      "Step 11000 | Val Loss: 2.17573| Best val loss: 2.17484 | Time:  1.7737 hrs\n",
      "Step 11500 | Val Loss: 2.17942| Best val loss: 2.17484 | Time:  1.8555 hrs\n",
      "Step 12000 | Val Loss: 2.17391| Best val loss: 2.17391 | Time:  1.9378 hrs\n",
      "Step 12500 | Val Loss: 2.16821| Best val loss: 2.16821 | Time:  2.0205 hrs\n",
      "Step 13000 | Val Loss: 2.16633| Best val loss: 2.16633 | Time:  2.1042 hrs\n",
      "Step 13500 | Val Loss: 2.16000| Best val loss: 2.16000 | Time:  2.1896 hrs\n",
      "Step 14000 | Val Loss: 2.16732| Best val loss: 2.16000 | Time:  2.2726 hrs\n",
      "Step 14500 | Val Loss: 2.16133| Best val loss: 2.16000 | Time:  2.3563 hrs\n",
      "Step 15000 | Val Loss: 2.16464| Best val loss: 2.16000 | Time:  2.4380 hrs\n",
      "Step 15500 | Val Loss: 2.16441| Best val loss: 2.16000 | Time:  2.5216 hrs\n",
      "Step 16000 | Val Loss: 2.16953| Best val loss: 2.16000 | Time:  2.6074 hrs\n",
      "Step 16500 | Val Loss: 2.15605| Best val loss: 2.15605 | Time:  2.6999 hrs\n",
      "Step 17000 | Val Loss: 2.15775| Best val loss: 2.15605 | Time:  2.8210 hrs\n",
      "Step 17500 | Val Loss: 2.16302| Best val loss: 2.15605 | Time:  2.9456 hrs\n",
      "Step 18000 | Val Loss: 2.15773| Best val loss: 2.15605 | Time:  3.0713 hrs\n",
      "Step 18500 | Val Loss: 2.15101| Best val loss: 2.15101 | Time:  3.1982 hrs\n",
      "Step 19000 | Val Loss: 2.14729| Best val loss: 2.14729 | Time:  3.3250 hrs\n",
      "Step 19500 | Val Loss: 2.14822| Best val loss: 2.14729 | Time:  3.4490 hrs\n",
      "Step 20000 | Val Loss: 2.14510| Best val loss: 2.14510 | Time:  3.5711 hrs\n",
      "Step 20500 | Val Loss: 2.14130| Best val loss: 2.14130 | Time:  3.6958 hrs\n",
      "Step 21000 | Val Loss: 2.14442| Best val loss: 2.14130 | Time:  3.8210 hrs\n",
      "Step 21500 | Val Loss: 2.14615| Best val loss: 2.14130 | Time:  3.9068 hrs\n",
      "Step 22000 | Val Loss: 2.14198| Best val loss: 2.14130 | Time:  3.9923 hrs\n",
      "Step 22500 | Val Loss: 2.13920| Best val loss: 2.13920 | Time:  4.0767 hrs\n",
      "Step 23000 | Val Loss: 2.14211| Best val loss: 2.13920 | Time:  4.1624 hrs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation():\n",
    "    total_loss = 0\n",
    "    for i, (src, tgt) in enumerate(val_loader):\n",
    "        max_src_len = min(token_limit, max([len(s) for s in src])) + 1   #need this to accomodate max_len = 1\n",
    "        max_tgt_len = min(token_limit, max([len(s) for s in tgt])) + 1\n",
    "        inputs = tokenizer(src, padding='max_length', truncation=True, max_length=max_src_len)\n",
    "        targets = tokenizer(tgt, padding='max_length', truncation=True, max_length=max_tgt_len)\n",
    "        input_ids, input_masks = inputs['input_ids'], inputs['attention_mask']\n",
    "        target_ids, target_masks = targets['input_ids'], targets['attention_mask']\n",
    "        for j in range(len(target_ids)):\n",
    "            target_ids[j].insert(0, 1)\n",
    "            target_masks[j].insert(0, 1)\n",
    "        input_ids, input_masks = torch.tensor(input_ids).to(device), torch.tensor(input_masks).to(device)\n",
    "        target_ids, target_masks = torch.tensor(target_ids).to(device), torch.tensor(target_masks).to(device)\n",
    "        loss = MT_model(input_ids, input_masks, target_ids, target_masks)\n",
    "        total_loss += loss\n",
    "    return total_loss / len(val_loader)\n",
    "        \n",
    "\n",
    "min_val_loss = 10000\n",
    "PATH = 'saved_models/prefix.pt'\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"------------------------EPOCH {epoch + 1}-------------------------------\")\n",
    "    t1 = time.time()\n",
    "    for i, (src, tgt) in enumerate(train_loader):\n",
    "        MT_model.zero_grad()\n",
    "        \n",
    "        max_src_len = min(token_limit, max([len(s) for s in src])) + 1   #need this to accomodate max_len = 1\n",
    "        max_tgt_len = min(token_limit, max([len(s) for s in tgt])) + 1\n",
    "        inputs = tokenizer(src, padding='max_length', truncation=True, max_length=max_src_len)\n",
    "        targets = tokenizer(tgt, padding='max_length', truncation=True, max_length=max_tgt_len)\n",
    "        input_ids, input_masks = inputs['input_ids'], inputs['attention_mask']\n",
    "        target_ids, target_masks = targets['input_ids'], targets['attention_mask']\n",
    "#         print(len(input_ids[0]))\n",
    "        for j in range(len(target_ids)):\n",
    "            target_ids[j].insert(0, 1)\n",
    "            target_masks[j].insert(0, 1)\n",
    "#         print(len(input_ids[0]))\n",
    "#         print(MT_model._model.config.max_position_embeddings)\n",
    "#         print(tgt[0], target_ids[0])\n",
    "        \n",
    "        input_ids, input_masks = torch.tensor(input_ids).to(device), torch.tensor(input_masks).to(device)\n",
    "        target_ids, target_masks = torch.tensor(target_ids).to(device), torch.tensor(target_masks).to(device)\n",
    "        loss = MT_model(input_ids, input_masks, target_ids, target_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1)%500 == 0:\n",
    "            t2 = time.time()\n",
    "            val_loss = validation()\n",
    "            if val_loss.item() < min_val_loss:\n",
    "                torch.save(MT_model.prefix.state_dict(), PATH)\n",
    "                min_val_loss = val_loss\n",
    "            print(f'Step {i+1} | Val Loss: {val_loss.item():.5f}| Best val loss: {min_val_loss:.5f} | Time: {(t2-t1)/3600 : .4f} hrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"saved_models/prefix.pt\"\n",
    "MT_model.prefix.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation():\n",
    "    total_loss = 0\n",
    "    for i, (src, tgt) in enumerate(val_loader):\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f\"Processing {i+1}....\")\n",
    "        max_src_len = min(token_limit, max([len(s) for s in src]))\n",
    "        max_tgt_len = min(token_limit, max([len(s) for s in tgt]))\n",
    "        inputs = tokenizer(src, padding='max_length', truncation=True, max_length=max_src_len)\n",
    "        targets = tokenizer(tgt, padding='max_length', truncation=True, max_length=max_tgt_len)\n",
    "        input_ids, input_masks = inputs['input_ids'], inputs['attention_mask']\n",
    "        target_ids, target_masks = targets['input_ids'], targets['attention_mask']\n",
    "        for j in range(len(target_ids)):\n",
    "            target_ids[j].insert(0, 1)\n",
    "            target_masks[j].insert(0, 1)\n",
    "        input_ids, input_masks = torch.tensor(input_ids).to(device), torch.tensor(input_masks).to(device)\n",
    "        target_ids, target_masks = torch.tensor(target_ids).to(device), torch.tensor(target_masks).to(device)\n",
    "        loss = MT_model(input_ids, input_masks, target_ids, target_masks)\n",
    "        total_loss += loss\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_translate(model, device, tokenizer, input_sent):\n",
    "    tok_output = tokenizer(input_sent)\n",
    "    input_ids = tok_output['input_ids']\n",
    "    input_mask = tok_output['attention_mask']\n",
    "    input_ids = torch.tensor(input_ids, device=device)\n",
    "    input_mask = torch.tensor(input_mask, device=device)\n",
    "    target_mask = torch.ones([1, 1], device=device)\n",
    "    past_key_values = model.encode(input_ids, input_mask)\n",
    "    print(past_key_values[0][0].shape)\n",
    "    start = [1]\n",
    "    gen = []\n",
    "    curr_token = None\n",
    "    while curr_token != 1:\n",
    "        tgt = torch.tensor(start, device=device)    \n",
    "        logits, past_key_values = model.decode(tgt, input_mask, target_mask, past_key_values)\n",
    "        print(past_key_values[0][0].shape)\n",
    "        logits = model.logSoftmax(logits.unsqueeze(0)).squeeze(0)\n",
    "        value, index = torch.max(logits, dim=1)\n",
    "        curr_token = index[0].item()\n",
    "        gen.append(curr_token)\n",
    "        start = [curr_token]\n",
    "        print(curr_token, value.item())\n",
    "        target_mask = torch.cat([target_mask, torch.ones([1, 1], device=device)], dim=1)\n",
    "    output_sent = tokenizer.decode(gen)\n",
    "    return output_sent\n",
    "\n",
    "sent = ['is it okay to have a bank account']\n",
    "greedy_translate(MT_model, device, tokenizer, sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>BLEU score</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "for src, tgt in test_loader:\n",
    "    \n",
    "\n",
    "ref_file = 'path/to/reference/translations.txt'\n",
    "with open(ref_file, 'r') as f:\n",
    "    refs = [line.strip() for line in f.readlines()]\n",
    "\n",
    "hyp_file = 'path/to/candidate/translations.txt'\n",
    "with open(hyp_file, 'r') as f:\n",
    "    hyps = [line.strip() for line in f.readlines()]\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
    "bleu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba129d3ed69af87e4eab8c2d4e17f59a651f25d15035729502e57aff85cb642c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
